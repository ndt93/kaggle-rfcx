{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.base import BaseEstimator\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting image-classifiers\n",
      "  Downloading image_classifiers-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting keras-applications<=1.0.8,>=1.0.7\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->image-classifiers) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->image-classifiers) (1.18.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->image-classifiers) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->image-classifiers) (1.18.5)\n",
      "Installing collected packages: keras-applications, image-classifiers\n",
      "Successfully installed image-classifiers-1.0.0 keras-applications-1.0.8\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install image-classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_models.keras import Classifiers\n",
    "\n",
    "ResNet34, preprocess_resnet34 = Classifiers.get('resnet34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_random_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BASE_INPUT_DIR = './input/'\n",
    "TRAIN_INPUT_DIR = os.path.join(BASE_INPUT_DIR, 'melspec-img')\n",
    "TEST_INPUT_DIR = os.path.join(BASE_INPUT_DIR, 'rfcx-species-audio-detection/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tp = pd.read_csv(os.path.join(BASE_INPUT_DIR, 'rfcx-species-audio-detection/train_tp.csv'))\n",
    "submission = pd.read_csv(os.path.join(BASE_INPUT_DIR, 'rfcx-species-audio-detection/sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Metrics & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _one_sample_positive_class_precisions(example):\n",
    "    y_true, y_pred = example\n",
    "    y_true = tf.cast(y_true > 0, dtype=tf.int32)\n",
    "\n",
    "    retrieved_classes = tf.argsort(y_pred, direction='DESCENDING')\n",
    "    class_rankings = tf.argsort(retrieved_classes)\n",
    "    retrieved_class_true = tf.gather(y_true, retrieved_classes)\n",
    "    retrieved_cumulative_hits = tf.math.cumsum(tf.cast(retrieved_class_true, tf.float32))\n",
    "\n",
    "    idx = tf.where(y_true)[:, 0]\n",
    "    i = tf.boolean_mask(class_rankings, y_true)\n",
    "    r = tf.gather(retrieved_cumulative_hits, i)\n",
    "    c = 1 + tf.cast(i, tf.float32)\n",
    "    precisions = r / c\n",
    "\n",
    "    dense = tf.scatter_nd(idx[:, None], precisions, [y_pred.shape[0]])\n",
    "    return dense\n",
    "\n",
    "\n",
    "class LWLRAP(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, num_classes, name='lwlrap'):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self._precisions = self.add_weight(\n",
    "            name='per_class_cumulative_precision',\n",
    "            shape=[num_classes],\n",
    "            initializer='zeros',\n",
    "        )\n",
    "\n",
    "        self._counts = self.add_weight(\n",
    "            name='per_class_cumulative_count',\n",
    "            shape=[num_classes],\n",
    "            initializer='zeros',\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        precisions = tf.map_fn(\n",
    "            fn=_one_sample_positive_class_precisions,\n",
    "            elems=(y_true, y_pred),\n",
    "            dtype=(tf.float32),\n",
    "        )\n",
    "\n",
    "        increments = tf.cast(precisions > 0, tf.float32)\n",
    "        total_increments = tf.reduce_sum(increments, axis=0)\n",
    "        total_precisions = tf.reduce_sum(precisions, axis=0)\n",
    "\n",
    "        self._precisions.assign_add(total_precisions)\n",
    "        self._counts.assign_add(total_increments)        \n",
    "\n",
    "    def result(self):\n",
    "        per_class_lwlrap = self._precisions / tf.maximum(self._counts, 1.0)\n",
    "        per_class_weight = self._counts / tf.reduce_sum(self._counts)\n",
    "        overall_lwlrap = tf.reduce_sum(per_class_lwlrap * per_class_weight)\n",
    "        return overall_lwlrap\n",
    "\n",
    "    def reset_states(self):\n",
    "        self._precisions.assign(self._precisions * 0)\n",
    "        self._counts.assign(self._counts * 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lsep_loss(num_classes, weights_list=None):\n",
    "    if weights_list is None:\n",
    "        weights_list = np.ones(num_classes, dtype=np.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def lsep_loss(y_true, y_pred):\n",
    "        batch_size = tf.math.floordiv(K.sum(K.exp(y_true - y_true)), num_classes)\n",
    "        \n",
    "        y_t = K.reshape(y_true, (batch_size, num_classes))\n",
    "        y_p = K.reshape(y_pred, (batch_size, num_classes))\n",
    "        M_unit = tf.ones((batch_size, num_classes)) \n",
    "        M1 = (M_unit - y_t) * K.reshape(K.tile(weights_list, [batch_size]), (batch_size, num_classes))\n",
    "       \n",
    "        M_pairwise = tf.einsum('ij,ik->ijk', M1, y_t)    # shape = (batch_size, num_classes, num_classes)\n",
    "        M_large = tf.einsum('ij,ik->ijk', M_unit, y_p)  # shape = (batch_size, num_classes, num_classes)\n",
    "\n",
    "        M_diff = K.exp(K.permute_dimensions(M_large, (0, 2, 1)) - M_large)  # shape = (batch_size, num_classes, num_classes)\n",
    "        M = M_pairwise * M_diff  # shape = (batch_size, num_classes, num_classes)\n",
    "\n",
    "        return K.mean(K.log(1 + K.sum(K.sum(M, 2), 1)))\n",
    "    \n",
    "    return lsep_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants & Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_SPECIES = 24\n",
    "\n",
    "IMG_SIZE = (224, 512, 3)\n",
    "IMG_HEIGHT = IMG_SIZE[0]\n",
    "IMG_WIDTH = IMG_SIZE[1]\n",
    "\n",
    "FMIN = 40.0\n",
    "FMAX = 24000.0\n",
    "\n",
    "SR = 48000\n",
    "N_MELS = 224\n",
    "N_FRAMES = 559\n",
    "\n",
    "CLIP_DURATION = 60\n",
    "SEGMENT_DURATION = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "cfg = {\n",
    "    'preprocess': {\n",
    "        'frame_size': 2048,\n",
    "        'hop_length': 512,\n",
    "        'sub_segment_duration': 6,\n",
    "\n",
    "        'spec_aug_prob': 0.7,\n",
    "        'spec_aug_erase_time': 50,\n",
    "        'spec_aug_erase_mel': 16,\n",
    "        'spec_aug_num_time_cuts': 2,\n",
    "        'spec_aug_num_freq_cuts': 4,\n",
    "        \n",
    "        'gauss_noise_prob': 0.7,\n",
    "        'gauss_noise_std': 0.5,\n",
    "        \n",
    "        'random_brightness': 0.3,\n",
    "        \n",
    "        'do_mixup': True,\n",
    "        'mixup_alpha': 0.2,\n",
    "    },\n",
    "    'training': {\n",
    "        'use_tpu': False,\n",
    "        'n_folds': 5,\n",
    "        'batch_size': 32,\n",
    "        'shuffle_buffer_size': 2000,\n",
    "        'steps_per_epoch': 64,\n",
    "\n",
    "        'frozen_learning_rate': 1e-2,\n",
    "        'num_unfreeze_layers': None,\n",
    "\n",
    "        'learning_rate': 1e-4,\n",
    "        'min_lr': 1e-7,\n",
    "    },\n",
    "    'model': {\n",
    "        'backbone_arch': 'resnet34',  # effnet_b0, effnet_b1, resnet50, resnet34\n",
    "        'backbone_preprocess': preprocess_resnet34,\n",
    "    },\n",
    "    'inference': {\n",
    "        'segment_stride': 6\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tpu_strategy = None\n",
    "\n",
    "if cfg['training']['use_tpu']:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "    \n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'idx': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'audio_wav': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'recording_id': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'cut_tmin': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'windows': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'species': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "}\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _parse_input_tfrec(example_proto):\n",
    "    sample = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    return {\n",
    "        'idx': tf.io.parse_tensor(sample['idx'], tf.int32),\n",
    "        'audio_wav': tf.io.parse_tensor(sample['audio_wav'], tf.float32),\n",
    "        'recording_id': tf.io.parse_tensor(sample['recording_id'], tf.string),\n",
    "        'cut_tmin': tf.io.parse_tensor(sample['cut_tmin'], tf.float32),\n",
    "        'windows': tf.io.parse_tensor(sample['windows'], tf.float32),\n",
    "        'species': tf.io.parse_tensor(sample['species'], tf.int32),\n",
    "    }\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _cut_wav(x, samples, is_train):\n",
    "    idx = x['idx']\n",
    "    cut_tmin = x['cut_tmin']\n",
    "    main_spid, main_tmin, main_tmax = samples[idx, 0], samples[idx, 1], samples[idx, 2]\n",
    "    main_spid = tf.cast(main_spid, tf.int32)\n",
    "    main_tmin = tf.cast(main_tmin, tf.float32)\n",
    "    main_tmax = tf.cast(main_tmax, tf.float32)\n",
    "    main_tmin -= cut_tmin\n",
    "    main_tmax -= cut_tmin\n",
    "    sub_segment_duration = cfg['preprocess']['sub_segment_duration']\n",
    "    \n",
    "    if is_train:\n",
    "        if main_tmax - main_tmin < sub_segment_duration:\n",
    "            min_left = tf.maximum(0.0, main_tmax - sub_segment_duration)\n",
    "            max_left = tf.minimum(main_tmin, SEGMENT_DURATION - sub_segment_duration)\n",
    "        else:\n",
    "            shrinkage = (main_tmax - main_tmin) - sub_segment_duration\n",
    "            min_left = main_tmin\n",
    "            max_left = main_tmin + shrinkage\n",
    "        left_cut = tf.random.uniform([], minval=min_left, maxval=max_left)\n",
    "    else:\n",
    "        if main_tmax - main_tmin < sub_segment_duration:\n",
    "            extension = tf.maximum(0.0, sub_segment_duration - (main_tmax - main_tmin))/2\n",
    "            left_extend = extension\n",
    "            if main_tmax + extension > SEGMENT_DURATION:\n",
    "                left_extend += main_tmax + extension - SEGMENT_DURATION\n",
    "            left_cut = tf.maximum(0.0, main_tmin - left_extend)\n",
    "        else:\n",
    "            shrinkage = (main_tmax - main_tmin) - sub_segment_duration\n",
    "            left_cut = main_tmin + shrinkage/2\n",
    "    \n",
    "    left_cut_sample = tf.cast(tf.floor(left_cut * SR), tf.int32)\n",
    "    right_cut_sample = left_cut_sample + sub_segment_duration*SR\n",
    "    x = x.copy()\n",
    "    x['audio_wav'] = tf.reshape(x['audio_wav'][left_cut_sample:right_cut_sample], [sub_segment_duration*SR])\n",
    "    x['windows'] = [[main_tmin, main_tmax]]\n",
    "    x['species'] = [main_spid]\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _wav_to_mel_spec(x):\n",
    "    frame_size = cfg['preprocess']['frame_size']\n",
    "    hop_length = cfg['preprocess']['hop_length']\n",
    "    \n",
    "    stfts = tf.signal.stft(\n",
    "        x[\"audio_wav\"],\n",
    "        frame_length=frame_size,\n",
    "        frame_step=hop_length,\n",
    "        fft_length=frame_size,\n",
    "        window_fn=tf.signal.hann_window,\n",
    "    )\n",
    "    hz_spec = tf.square(tf.abs(stfts))\n",
    "\n",
    "    n_hz_spec_bins = tf.shape(stfts)[-1]\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(N_MELS, n_hz_spec_bins, SR, FMIN, FMAX)\n",
    "    mel_spec = tf.tensordot(hz_spec, linear_to_mel_weight_matrix, 1)\n",
    "    mel_spec.set_shape(hz_spec.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
    "    log_mel_spec = tf.math.log(mel_spec + 1e-6)/tf.math.log(10.0)\n",
    "\n",
    "    y = {\n",
    "        'mel_spec': tf.transpose(log_mel_spec)\n",
    "    }\n",
    "    y.update(x)\n",
    "    return y\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _create_labels(x):\n",
    "    species, _ = tf.unique(tf.cast(x['species'], dtype=tf.int64))\n",
    "    species = tf.sort(species)\n",
    "    n_labels = tf.shape(species)[0]\n",
    "    species = tf.reshape(species, (-1, 1))\n",
    "    return tf.sparse.to_dense(tf.sparse.SparseTensor(species, tf.ones([tf.shape(species)[0]]), [NUM_SPECIES]))\n",
    "\n",
    "\n",
    "def _create_idx_filter(indices):\n",
    "    @tf.function\n",
    "    def _filter_func(x):\n",
    "        return tf.reduce_any(indices == x['idx'])\n",
    "    return _filter_func\n",
    "\n",
    "\n",
    "def _filter_indices(dataset, indices):\n",
    "    return dataset.filter(_create_idx_filter(indices))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def augment_img(image):\n",
    "    \n",
    "    @tf.function\n",
    "    def _specaugment(image):\n",
    "        erase_time = cfg['preprocess']['spec_aug_erase_time']\n",
    "        erase_mel = cfg['preprocess']['spec_aug_erase_mel']\n",
    "        num_time_cuts = cfg['preprocess']['spec_aug_num_time_cuts']\n",
    "        num_freq_cuts = cfg['preprocess']['spec_aug_num_freq_cuts']\n",
    "        img_height = tf.shape(image)[0]\n",
    "        img_width = tf.shape(image)[1]\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "\n",
    "        xoff = tf.random.uniform([num_time_cuts], minval=erase_time//2, maxval=img_width - erase_time//2, dtype=tf.int32)\n",
    "        xsize = tf.random.uniform([num_time_cuts], minval=erase_time//2, maxval=erase_time, dtype=tf.int32)\n",
    "        yoff = tf.random.uniform([num_freq_cuts], minval=erase_mel//2, maxval=img_height-erase_mel//2, dtype=tf.int32)\n",
    "        ysize = tf.random.uniform([num_freq_cuts], minval=erase_mel//2, maxval=erase_mel, dtype=tf.int32)\n",
    "\n",
    "        for i in range(num_time_cuts):\n",
    "            image = tfa.image.cutout(image, [img_height, xsize[i]], offset=[img_height//2, xoff[i]])\n",
    "        for i in range(num_freq_cuts):\n",
    "            image = tfa.image.cutout(image, [ysize[i], img_width], offset=[yoff[i], img_width//2])\n",
    "\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "        return image\n",
    "    \n",
    "    gauss_noise = tf.keras.layers.GaussianNoise(cfg['preprocess']['gauss_noise_std']) \n",
    "    image = tf.cond(\n",
    "        tf.random.uniform([]) < cfg['preprocess']['gauss_noise_prob'],\n",
    "        lambda: gauss_noise(image, training=True), lambda: image\n",
    "    )\n",
    "    image = tf.image.random_brightness(image, cfg['preprocess']['random_brightness'])\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.cond(\n",
    "        tf.random.uniform([]) < cfg['preprocess']['spec_aug_prob'],\n",
    "        lambda: _specaugment(image), lambda: image\n",
    "    )\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _mixup(inp, targ):\n",
    "    indices = tf.range(len(inp))\n",
    "    indices = tf.random.shuffle(indices)\n",
    "    sinp = tf.gather(inp, indices, axis=0)\n",
    "    starg = tf.gather(targ, indices, axis=0)\n",
    "\n",
    "    alpha = cfg['preprocess']['mixup_alpha']\n",
    "    t = tf.compat.v1.distributions.Beta(alpha, alpha).sample([len(inp)])\n",
    "    tx = tf.reshape(t, [-1, 1, 1, 1])\n",
    "    ty = tf.reshape(t, [-1, 1])\n",
    "    x = inp * tx + sinp * (1 - tx)\n",
    "    y = targ * ty + starg * (1 - ty)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def preprocess_img(img, is_train):\n",
    "    img.set_shape([N_MELS, N_FRAMES])\n",
    "    img = tf.expand_dims(img, axis=-1)\n",
    "    img = tf.image.resize(img, IMG_SIZE[:-1])\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    if is_train:\n",
    "        img = augment_img(img)\n",
    "    img_min = tf.reduce_min(img)\n",
    "    img_max = tf.reduce_max(img)\n",
    "    img = (img - img_min)/(img_max - img_min)*255\n",
    "    img = tf.image.grayscale_to_rgb(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def build_dataset(indices, samples, is_train):\n",
    "    \n",
    "    @tf.function\n",
    "    def _split_img_label(x):\n",
    "        return x['mel_spec'], _create_labels(x)\n",
    "    \n",
    "    @tf.function\n",
    "    def _cut_sub_segment(x):\n",
    "        return _cut_wav(x, samples, is_train)\n",
    "    \n",
    "    samples = samples[['species_id', 't_min', 't_max']].values\n",
    "    filepath = os.path.join(TRAIN_INPUT_DIR, 'train.cut_audio.tfrecord')\n",
    "    ds = tf.data.TFRecordDataset(filepath)\n",
    "    ds = ds.map(_parse_input_tfrec, num_parallel_calls=AUTOTUNE)\n",
    "    ds = _filter_indices(ds, indices)\n",
    "    ds = ds.cache()\n",
    "    if is_train:\n",
    "        ds = ds.shuffle(buffer_size=cfg['training']['shuffle_buffer_size'])\n",
    "        ds = ds.repeat()\n",
    "    ds = ds.map(_cut_sub_segment, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(_wav_to_mel_spec, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(_split_img_label, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(lambda img, l: (preprocess_img(img, is_train), l), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(cfg['training']['batch_size'])\n",
    "    if is_train and cfg['preprocess']['do_mixup']:\n",
    "        ds = ds.map(_mixup, num_parallel_calls=AUTOTUNE)\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ModelNN(BaseEstimator):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes=NUM_SPECIES,\n",
    "        backbone_arch=cfg['model']['backbone_arch'],\n",
    "    ):\n",
    "        self.n_classes = n_classes\n",
    "        self.backbone_arch = backbone_arch\n",
    "        self.estimator_ = None\n",
    "        \n",
    "    def compile_model(self, model, is_frozen):\n",
    "        if is_frozen:\n",
    "            learning_rate = cfg['training']['frozen_learning_rate']\n",
    "        else:\n",
    "            learning_rate = cfg['training']['learning_rate']\n",
    "            \n",
    "        lr_schedule = tf.keras.experimental.CosineDecayRestarts(\n",
    "            learning_rate, 640, t_mul=2.0, m_mul=1.0, alpha=1e-8, name='consine_decay'\n",
    "        )\n",
    "        model.compile(\n",
    "#             optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "            loss=get_lsep_loss(NUM_SPECIES),\n",
    "            metrics=[LWLRAP(self.n_classes), keras.metrics.AUC(multi_label=True, name='auc'), 'accuracy']\n",
    "        )\n",
    "        return model\n",
    "        \n",
    "    def build_model(self, freeze_backbone):\n",
    "        inputs = keras.layers.Input(shape=IMG_SIZE)\n",
    "\n",
    "        if self.backbone_arch == 'effnet_b0':\n",
    "            model_backbone = keras.applications.EfficientNetB0(include_top=False, input_tensor=inputs)\n",
    "        elif self.backbone_arch == 'effnet_b1':\n",
    "            model_backbone = keras.applications.EfficientNetB1(include_top=False, input_tensor=inputs)\n",
    "        elif self.backbone_arch == 'resnet50':\n",
    "            model_backbone = keras.applications.ResNet50(include_top=False, input_tensor=inputs)\n",
    "        elif self.backbone_arch == 'resnet34':\n",
    "            model_backbone = ResNet34(include_top=False, input_tensor=inputs, weights='imagenet')\n",
    "        else:\n",
    "            raise Exception(f'Unsupported backbone arch: {self.backbone_arch}')\n",
    "            \n",
    "        if freeze_backbone:\n",
    "            model_backbone.trainable = False\n",
    "        \n",
    "        x = keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(model_backbone.output)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        top_dropout_rate = 0.4\n",
    "        x = keras.layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "        outputs = tfa.layers.WeightNormalization(keras.layers.Dense(self.n_classes, activation='sigmoid'), name=\"pred\")(x)\n",
    "\n",
    "        model = keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
    "        return self.compile_model(model, freeze_backbone)\n",
    "    \n",
    "    def unfreeze_backbone(self, num_layers=cfg['training']['num_unfreeze_layers']):\n",
    "        model = self.estimator_\n",
    "        selected_layers = model.layers[-num_layers:] if num_layers is not None else model.layers\n",
    "        for layer in selected_layers:\n",
    "            if not isinstance(layer, keras.layers.BatchNormalization):\n",
    "                layer.trainable = True\n",
    "        self.estimator_ = self.compile_model(model, False)\n",
    "  \n",
    "    def fit(self, train_dataset, val_dataset, n_epochs=25, weights_suffix='', with_callbacks=True):\n",
    "        if self.estimator_ is None:\n",
    "            if cfg['training']['use_tpu']:\n",
    "                with tpu_strategy.scope():\n",
    "                    self.estimator_ = self.build_model(freeze_backbone=True)\n",
    "            else:\n",
    "                self.estimator_ = self.build_model(freeze_backbone=True)\n",
    "        \n",
    "        model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "            f'./weights{weights_suffix}.h5',\n",
    "            monitor='val_lwlrap',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode='max'\n",
    "        )\n",
    "        lr_scheduler = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_lwlrap',\n",
    "            patience=5,\n",
    "            mode='max',\n",
    "            min_lr=cfg['training']['min_lr'],\n",
    "            verbose=2\n",
    "        )\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_lwlrap',\n",
    "            patience=15,\n",
    "            mode='max',\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        logs_cb = tf.keras.callbacks.CSVLogger(f'logs/metrics{weights_suffix}.csv')\n",
    "#         callbacks = [model_checkpoint, lr_scheduler, early_stopping, logs_cb]\n",
    "        callbacks = [model_checkpoint, early_stopping, logs_cb]\n",
    "\n",
    "        return self.estimator_.fit(\n",
    "            train_dataset,\n",
    "            epochs=n_epochs,\n",
    "            callbacks=callbacks if with_callbacks else None,\n",
    "            validation_data=val_dataset,\n",
    "            steps_per_epoch=cfg['training']['steps_per_epoch'],\n",
    "            verbose=2\n",
    "        )\n",
    "    \n",
    "    def predict(self, img):\n",
    "        return self.estimator_.predict(img)\n",
    "    \n",
    "    def load_weights(self, checkpoint_path):\n",
    "        if self.estimator_ is None:\n",
    "            self.estimator_ = self.build_model(freeze_backbone=False)\n",
    "            self.unfreeze_backbone()\n",
    "        self.estimator_.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_val_metrics(history):\n",
    "    max_idx = np.argmax(history.history['val_lwlrap'])\n",
    "    print(\n",
    "        history.history['val_lwlrap'][max_idx],\n",
    "        history.history['val_auc'][max_idx],\n",
    "        history.history['val_accuracy'][max_idx],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = train_tp.copy()\n",
    "targets = train_tp['species_id'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs: {'preprocess': {'frame_size': 2048, 'hop_length': 512, 'sub_segment_duration': 6, 'spec_aug_prob': 0.7, 'spec_aug_erase_time': 50, 'spec_aug_erase_mel': 16, 'spec_aug_num_time_cuts': 2, 'spec_aug_num_freq_cuts': 4, 'gauss_noise_prob': 0.7, 'gauss_noise_std': 0.5, 'random_brightness': 0.3, 'do_mixup': True, 'mixup_alpha': 0.2}, 'training': {'use_tpu': False, 'n_folds': 5, 'batch_size': 32, 'shuffle_buffer_size': 2000, 'steps_per_epoch': 64, 'frozen_learning_rate': 0.01, 'num_unfreeze_layers': None, 'learning_rate': 0.0001, 'min_lr': 1e-07}, 'model': {'backbone_arch': 'resnet34', 'backbone_preprocess': <function preprocess_input at 0x7f72b8771560>}, 'inference': {'segment_stride': 6}}\n",
      "* [0] Train with frozen backbone\n",
      "Downloading data from https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5\n",
      "85524480/85521592 [==============================] - 8s 0us/step\n",
      "Epoch 1/5\n",
      "64/64 - 56s - loss: 3.0312 - lwlrap: 0.2771 - auc: 0.6133 - accuracy: 0.1328 - val_loss: 2.9844 - val_lwlrap: 0.3305 - val_auc: 0.7933 - val_accuracy: 0.1680\n",
      "Epoch 2/5\n",
      "64/64 - 57s - loss: 2.9418 - lwlrap: 0.3156 - auc: 0.6421 - accuracy: 0.1719 - val_loss: 2.9191 - val_lwlrap: 0.3755 - val_auc: 0.8052 - val_accuracy: 0.2008\n",
      "Epoch 3/5\n",
      "64/64 - 58s - loss: 2.9055 - lwlrap: 0.3337 - auc: 0.6583 - accuracy: 0.2021 - val_loss: 2.8417 - val_lwlrap: 0.4282 - val_auc: 0.8193 - val_accuracy: 0.2336\n",
      "Epoch 4/5\n",
      "64/64 - 55s - loss: 2.8928 - lwlrap: 0.3410 - auc: 0.6602 - accuracy: 0.2280 - val_loss: 2.8228 - val_lwlrap: 0.4504 - val_auc: 0.8252 - val_accuracy: 0.2705\n",
      "Epoch 5/5\n",
      "64/64 - 55s - loss: 2.8767 - lwlrap: 0.3553 - auc: 0.6701 - accuracy: 0.2407 - val_loss: 2.7983 - val_lwlrap: 0.4783 - val_auc: 0.8310 - val_accuracy: 0.3115\n",
      "0.4782845377922058 0.8309624791145325 0.31147539615631104\n",
      "** [0] Train with full model\n",
      "Epoch 1/100\n",
      "64/64 - 58s - loss: 2.9011 - lwlrap: 0.3233 - auc: 0.6521 - accuracy: 0.1855 - val_loss: 2.7645 - val_lwlrap: 0.4162 - val_auc: 0.8396 - val_accuracy: 0.2336\n",
      "Epoch 2/100\n",
      "64/64 - 56s - loss: 2.8019 - lwlrap: 0.3884 - auc: 0.6901 - accuracy: 0.3022 - val_loss: 2.7102 - val_lwlrap: 0.5215 - val_auc: 0.8714 - val_accuracy: 0.3279\n",
      "Epoch 3/100\n",
      "64/64 - 56s - loss: 2.7622 - lwlrap: 0.4184 - auc: 0.7049 - accuracy: 0.3726 - val_loss: 2.6968 - val_lwlrap: 0.5701 - val_auc: 0.8884 - val_accuracy: 0.4057\n",
      "Epoch 4/100\n",
      "64/64 - 56s - loss: 2.7397 - lwlrap: 0.4353 - auc: 0.7118 - accuracy: 0.4014 - val_loss: 2.6249 - val_lwlrap: 0.6351 - val_auc: 0.9120 - val_accuracy: 0.4631\n",
      "Epoch 5/100\n",
      "64/64 - 57s - loss: 2.7012 - lwlrap: 0.4573 - auc: 0.7199 - accuracy: 0.4375 - val_loss: 2.6081 - val_lwlrap: 0.6171 - val_auc: 0.9214 - val_accuracy: 0.4344\n",
      "Epoch 6/100\n",
      "64/64 - 57s - loss: 2.6877 - lwlrap: 0.4618 - auc: 0.7273 - accuracy: 0.4404 - val_loss: 2.5933 - val_lwlrap: 0.6373 - val_auc: 0.9227 - val_accuracy: 0.4508\n",
      "Epoch 7/100\n",
      "64/64 - 57s - loss: 2.6782 - lwlrap: 0.4767 - auc: 0.7307 - accuracy: 0.4702 - val_loss: 2.5875 - val_lwlrap: 0.6382 - val_auc: 0.9282 - val_accuracy: 0.4508\n",
      "Epoch 8/100\n",
      "64/64 - 55s - loss: 2.6794 - lwlrap: 0.4726 - auc: 0.7320 - accuracy: 0.4702 - val_loss: 2.6117 - val_lwlrap: 0.6060 - val_auc: 0.9243 - val_accuracy: 0.4385\n",
      "Epoch 9/100\n",
      "64/64 - 55s - loss: 2.7114 - lwlrap: 0.4476 - auc: 0.7225 - accuracy: 0.4229 - val_loss: 2.6150 - val_lwlrap: 0.5953 - val_auc: 0.9193 - val_accuracy: 0.4016\n",
      "Epoch 10/100\n",
      "64/64 - 56s - loss: 2.6899 - lwlrap: 0.4682 - auc: 0.7279 - accuracy: 0.4697 - val_loss: 2.6182 - val_lwlrap: 0.6431 - val_auc: 0.9227 - val_accuracy: 0.4877\n",
      "Epoch 11/100\n",
      "64/64 - 56s - loss: 2.6585 - lwlrap: 0.4841 - auc: 0.7383 - accuracy: 0.5059 - val_loss: 2.5904 - val_lwlrap: 0.6726 - val_auc: 0.9313 - val_accuracy: 0.5328\n",
      "Epoch 12/100\n",
      "64/64 - 56s - loss: 2.6393 - lwlrap: 0.4957 - auc: 0.7341 - accuracy: 0.5220 - val_loss: 2.5739 - val_lwlrap: 0.6875 - val_auc: 0.9373 - val_accuracy: 0.5492\n",
      "Epoch 13/100\n",
      "64/64 - 57s - loss: 2.6307 - lwlrap: 0.5067 - auc: 0.7431 - accuracy: 0.5474 - val_loss: 2.5288 - val_lwlrap: 0.7357 - val_auc: 0.9392 - val_accuracy: 0.6066\n",
      "Epoch 14/100\n",
      "64/64 - 56s - loss: 2.6190 - lwlrap: 0.5212 - auc: 0.7462 - accuracy: 0.5762 - val_loss: 2.5291 - val_lwlrap: 0.7164 - val_auc: 0.9493 - val_accuracy: 0.5656\n",
      "Epoch 15/100\n",
      "64/64 - 57s - loss: 2.6123 - lwlrap: 0.5200 - auc: 0.7451 - accuracy: 0.5767 - val_loss: 2.5327 - val_lwlrap: 0.7501 - val_auc: 0.9506 - val_accuracy: 0.6352\n",
      "Epoch 16/100\n",
      "64/64 - 56s - loss: 2.5936 - lwlrap: 0.5372 - auc: 0.7510 - accuracy: 0.6250 - val_loss: 2.5069 - val_lwlrap: 0.7458 - val_auc: 0.9501 - val_accuracy: 0.6148\n",
      "Epoch 17/100\n",
      "64/64 - 57s - loss: 2.5789 - lwlrap: 0.5455 - auc: 0.7550 - accuracy: 0.6318 - val_loss: 2.4962 - val_lwlrap: 0.7591 - val_auc: 0.9532 - val_accuracy: 0.6311\n",
      "Epoch 18/100\n",
      "64/64 - 57s - loss: 2.5694 - lwlrap: 0.5510 - auc: 0.7555 - accuracy: 0.6475 - val_loss: 2.4736 - val_lwlrap: 0.7852 - val_auc: 0.9559 - val_accuracy: 0.6680\n",
      "Epoch 19/100\n",
      "64/64 - 56s - loss: 2.5589 - lwlrap: 0.5549 - auc: 0.7569 - accuracy: 0.6548 - val_loss: 2.4716 - val_lwlrap: 0.7748 - val_auc: 0.9568 - val_accuracy: 0.6516\n",
      "Epoch 20/100\n",
      "64/64 - 56s - loss: 2.5546 - lwlrap: 0.5578 - auc: 0.7562 - accuracy: 0.6650 - val_loss: 2.4690 - val_lwlrap: 0.7800 - val_auc: 0.9573 - val_accuracy: 0.6598\n",
      "Epoch 21/100\n",
      "64/64 - 56s - loss: 2.5446 - lwlrap: 0.5592 - auc: 0.7577 - accuracy: 0.6714 - val_loss: 2.4681 - val_lwlrap: 0.7789 - val_auc: 0.9551 - val_accuracy: 0.6557\n",
      "Epoch 22/100\n",
      "64/64 - 57s - loss: 2.5542 - lwlrap: 0.5622 - auc: 0.7606 - accuracy: 0.6660 - val_loss: 2.4618 - val_lwlrap: 0.7913 - val_auc: 0.9595 - val_accuracy: 0.6721\n",
      "Epoch 23/100\n",
      "64/64 - 56s - loss: 2.5479 - lwlrap: 0.5551 - auc: 0.7600 - accuracy: 0.6553 - val_loss: 2.4628 - val_lwlrap: 0.7873 - val_auc: 0.9581 - val_accuracy: 0.6680\n",
      "Epoch 24/100\n",
      "64/64 - 56s - loss: 2.5569 - lwlrap: 0.5491 - auc: 0.7514 - accuracy: 0.6514 - val_loss: 2.4896 - val_lwlrap: 0.7691 - val_auc: 0.9563 - val_accuracy: 0.6475\n",
      "Epoch 25/100\n",
      "64/64 - 56s - loss: 2.5753 - lwlrap: 0.5401 - auc: 0.7557 - accuracy: 0.6221 - val_loss: 2.5200 - val_lwlrap: 0.7350 - val_auc: 0.9445 - val_accuracy: 0.5984\n",
      "Epoch 26/100\n",
      "64/64 - 56s - loss: 2.5940 - lwlrap: 0.5272 - auc: 0.7461 - accuracy: 0.6094 - val_loss: 2.4868 - val_lwlrap: 0.7346 - val_auc: 0.9460 - val_accuracy: 0.5861\n",
      "Epoch 27/100\n",
      "64/64 - 56s - loss: 2.5677 - lwlrap: 0.5420 - auc: 0.7470 - accuracy: 0.6528 - val_loss: 2.4799 - val_lwlrap: 0.7667 - val_auc: 0.9587 - val_accuracy: 0.6434\n",
      "Epoch 28/100\n",
      "64/64 - 57s - loss: 2.5581 - lwlrap: 0.5525 - auc: 0.7602 - accuracy: 0.6499 - val_loss: 2.4572 - val_lwlrap: 0.7958 - val_auc: 0.9598 - val_accuracy: 0.6803\n",
      "Epoch 29/100\n",
      "64/64 - 56s - loss: 2.5402 - lwlrap: 0.5550 - auc: 0.7530 - accuracy: 0.6777 - val_loss: 2.4563 - val_lwlrap: 0.7844 - val_auc: 0.9667 - val_accuracy: 0.6557\n",
      "Epoch 30/100\n",
      "64/64 - 56s - loss: 2.5400 - lwlrap: 0.5563 - auc: 0.7588 - accuracy: 0.6665 - val_loss: 2.4674 - val_lwlrap: 0.7859 - val_auc: 0.9604 - val_accuracy: 0.6639\n",
      "Epoch 31/100\n",
      "64/64 - 56s - loss: 2.5251 - lwlrap: 0.5587 - auc: 0.7539 - accuracy: 0.6665 - val_loss: 2.4568 - val_lwlrap: 0.7769 - val_auc: 0.9609 - val_accuracy: 0.6393\n",
      "Epoch 32/100\n",
      "64/64 - 57s - loss: 2.5298 - lwlrap: 0.5631 - auc: 0.7614 - accuracy: 0.6699 - val_loss: 2.4321 - val_lwlrap: 0.8088 - val_auc: 0.9693 - val_accuracy: 0.6844\n",
      "Epoch 33/100\n",
      "64/64 - 56s - loss: 2.5225 - lwlrap: 0.5551 - auc: 0.7512 - accuracy: 0.6724 - val_loss: 2.4887 - val_lwlrap: 0.7759 - val_auc: 0.9570 - val_accuracy: 0.6680\n",
      "Epoch 34/100\n",
      "64/64 - 57s - loss: 2.5021 - lwlrap: 0.5715 - auc: 0.7563 - accuracy: 0.7061 - val_loss: 2.4411 - val_lwlrap: 0.8099 - val_auc: 0.9691 - val_accuracy: 0.6967\n",
      "Epoch 35/100\n",
      "64/64 - 56s - loss: 2.5047 - lwlrap: 0.5792 - auc: 0.7619 - accuracy: 0.7075 - val_loss: 2.4436 - val_lwlrap: 0.7999 - val_auc: 0.9592 - val_accuracy: 0.6844\n",
      "Epoch 36/100\n",
      "64/64 - 57s - loss: 2.5006 - lwlrap: 0.5778 - auc: 0.7552 - accuracy: 0.7178 - val_loss: 2.4170 - val_lwlrap: 0.8260 - val_auc: 0.9719 - val_accuracy: 0.7172\n",
      "Epoch 37/100\n",
      "64/64 - 56s - loss: 2.4979 - lwlrap: 0.5772 - auc: 0.7573 - accuracy: 0.7334 - val_loss: 2.4336 - val_lwlrap: 0.8010 - val_auc: 0.9685 - val_accuracy: 0.6762\n",
      "Epoch 38/100\n",
      "64/64 - 57s - loss: 2.4851 - lwlrap: 0.5839 - auc: 0.7630 - accuracy: 0.7407 - val_loss: 2.4284 - val_lwlrap: 0.8320 - val_auc: 0.9660 - val_accuracy: 0.7418\n",
      "Epoch 39/100\n",
      "64/64 - 56s - loss: 2.4794 - lwlrap: 0.5893 - auc: 0.7630 - accuracy: 0.7314 - val_loss: 2.4170 - val_lwlrap: 0.8230 - val_auc: 0.9712 - val_accuracy: 0.7049\n",
      "Epoch 40/100\n",
      "64/64 - 57s - loss: 2.4829 - lwlrap: 0.5912 - auc: 0.7679 - accuracy: 0.7554 - val_loss: 2.4024 - val_lwlrap: 0.8528 - val_auc: 0.9725 - val_accuracy: 0.7541\n",
      "Epoch 41/100\n",
      "64/64 - 56s - loss: 2.4767 - lwlrap: 0.5910 - auc: 0.7675 - accuracy: 0.7563 - val_loss: 2.4079 - val_lwlrap: 0.8306 - val_auc: 0.9764 - val_accuracy: 0.7131\n",
      "Epoch 42/100\n",
      "64/64 - 56s - loss: 2.4849 - lwlrap: 0.5892 - auc: 0.7640 - accuracy: 0.7461 - val_loss: 2.4030 - val_lwlrap: 0.8380 - val_auc: 0.9755 - val_accuracy: 0.7295\n",
      "Epoch 43/100\n",
      "64/64 - 56s - loss: 2.4726 - lwlrap: 0.5889 - auc: 0.7615 - accuracy: 0.7432 - val_loss: 2.4138 - val_lwlrap: 0.8388 - val_auc: 0.9736 - val_accuracy: 0.7418\n",
      "Epoch 44/100\n",
      "64/64 - 56s - loss: 2.4636 - lwlrap: 0.6022 - auc: 0.7663 - accuracy: 0.7705 - val_loss: 2.4055 - val_lwlrap: 0.8344 - val_auc: 0.9742 - val_accuracy: 0.7213\n",
      "Epoch 45/100\n",
      "64/64 - 57s - loss: 2.4573 - lwlrap: 0.5979 - auc: 0.7648 - accuracy: 0.7656 - val_loss: 2.4128 - val_lwlrap: 0.8544 - val_auc: 0.9762 - val_accuracy: 0.7582\n",
      "Epoch 46/100\n",
      "64/64 - 56s - loss: 2.4585 - lwlrap: 0.6048 - auc: 0.7685 - accuracy: 0.7778 - val_loss: 2.4008 - val_lwlrap: 0.8482 - val_auc: 0.9786 - val_accuracy: 0.7418\n",
      "Epoch 47/100\n",
      "64/64 - 56s - loss: 2.4649 - lwlrap: 0.5920 - auc: 0.7591 - accuracy: 0.7622 - val_loss: 2.4066 - val_lwlrap: 0.8420 - val_auc: 0.9747 - val_accuracy: 0.7377\n",
      "Epoch 48/100\n",
      "64/64 - 56s - loss: 2.4628 - lwlrap: 0.5983 - auc: 0.7614 - accuracy: 0.7617 - val_loss: 2.3992 - val_lwlrap: 0.8461 - val_auc: 0.9773 - val_accuracy: 0.7459\n",
      "Epoch 49/100\n",
      "64/64 - 56s - loss: 2.4580 - lwlrap: 0.6018 - auc: 0.7643 - accuracy: 0.7852 - val_loss: 2.3997 - val_lwlrap: 0.8522 - val_auc: 0.9760 - val_accuracy: 0.7541\n",
      "Epoch 50/100\n",
      "64/64 - 57s - loss: 2.4591 - lwlrap: 0.6066 - auc: 0.7689 - accuracy: 0.7871 - val_loss: 2.4001 - val_lwlrap: 0.8579 - val_auc: 0.9746 - val_accuracy: 0.7705\n",
      "Epoch 51/100\n",
      "64/64 - 56s - loss: 2.4460 - lwlrap: 0.6130 - auc: 0.7669 - accuracy: 0.8027 - val_loss: 2.3992 - val_lwlrap: 0.8546 - val_auc: 0.9770 - val_accuracy: 0.7623\n",
      "Epoch 52/100\n",
      "64/64 - 56s - loss: 2.4465 - lwlrap: 0.6130 - auc: 0.7685 - accuracy: 0.7935 - val_loss: 2.3975 - val_lwlrap: 0.8529 - val_auc: 0.9776 - val_accuracy: 0.7582\n",
      "Epoch 53/100\n",
      "64/64 - 56s - loss: 2.4485 - lwlrap: 0.6071 - auc: 0.7626 - accuracy: 0.7798 - val_loss: 2.3984 - val_lwlrap: 0.8505 - val_auc: 0.9762 - val_accuracy: 0.7541\n",
      "Epoch 54/100\n",
      "64/64 - 56s - loss: 2.4492 - lwlrap: 0.6070 - auc: 0.7626 - accuracy: 0.7842 - val_loss: 2.3984 - val_lwlrap: 0.8525 - val_auc: 0.9763 - val_accuracy: 0.7582\n",
      "Epoch 55/100\n",
      "64/64 - 56s - loss: 2.4619 - lwlrap: 0.6005 - auc: 0.7680 - accuracy: 0.7646 - val_loss: 2.4589 - val_lwlrap: 0.8040 - val_auc: 0.9685 - val_accuracy: 0.6926\n",
      "Epoch 56/100\n",
      "64/64 - 56s - loss: 2.5021 - lwlrap: 0.5739 - auc: 0.7589 - accuracy: 0.7261 - val_loss: 2.4396 - val_lwlrap: 0.8122 - val_auc: 0.9725 - val_accuracy: 0.7008\n",
      "Epoch 57/100\n",
      "64/64 - 56s - loss: 2.5021 - lwlrap: 0.5812 - auc: 0.7566 - accuracy: 0.7393 - val_loss: 2.4287 - val_lwlrap: 0.8201 - val_auc: 0.9649 - val_accuracy: 0.7131\n",
      "Epoch 58/100\n",
      "64/64 - 56s - loss: 2.4997 - lwlrap: 0.5719 - auc: 0.7528 - accuracy: 0.7334 - val_loss: 2.4447 - val_lwlrap: 0.8279 - val_auc: 0.9714 - val_accuracy: 0.7336\n",
      "Epoch 59/100\n",
      "64/64 - 56s - loss: 2.4922 - lwlrap: 0.5818 - auc: 0.7587 - accuracy: 0.7446 - val_loss: 2.4570 - val_lwlrap: 0.8136 - val_auc: 0.9651 - val_accuracy: 0.7090\n",
      "Epoch 60/100\n",
      "64/64 - 56s - loss: 2.4855 - lwlrap: 0.5990 - auc: 0.7683 - accuracy: 0.7515 - val_loss: 2.4203 - val_lwlrap: 0.8367 - val_auc: 0.9711 - val_accuracy: 0.7377\n",
      "Epoch 61/100\n",
      "64/64 - 56s - loss: 2.4677 - lwlrap: 0.5929 - auc: 0.7614 - accuracy: 0.7695 - val_loss: 2.4380 - val_lwlrap: 0.8276 - val_auc: 0.9509 - val_accuracy: 0.7377\n",
      "Epoch 62/100\n",
      "64/64 - 56s - loss: 2.4800 - lwlrap: 0.5874 - auc: 0.7553 - accuracy: 0.7681 - val_loss: 2.4265 - val_lwlrap: 0.8434 - val_auc: 0.9673 - val_accuracy: 0.7500\n",
      "Epoch 63/100\n",
      "64/64 - 56s - loss: 2.4657 - lwlrap: 0.5990 - auc: 0.7670 - accuracy: 0.7778 - val_loss: 2.4159 - val_lwlrap: 0.8289 - val_auc: 0.9732 - val_accuracy: 0.7213\n",
      "Epoch 64/100\n",
      "64/64 - 56s - loss: 2.4622 - lwlrap: 0.5977 - auc: 0.7593 - accuracy: 0.7935 - val_loss: 2.4218 - val_lwlrap: 0.8228 - val_auc: 0.9648 - val_accuracy: 0.7213\n",
      "Epoch 65/100\n",
      "64/64 - 56s - loss: 2.4644 - lwlrap: 0.5989 - auc: 0.7653 - accuracy: 0.7896 - val_loss: 2.4236 - val_lwlrap: 0.8332 - val_auc: 0.9671 - val_accuracy: 0.7418\n",
      "0.8579318523406982 0.9745777249336243 0.7704917788505554\n",
      "* [1] Train with frozen backbone\n",
      "Epoch 1/5\n",
      "64/64 - 60s - loss: 3.0325 - lwlrap: 0.2789 - auc: 0.6116 - accuracy: 0.1377 - val_loss: 2.9805 - val_lwlrap: 0.3290 - val_auc: 0.8120 - val_accuracy: 0.1770\n",
      "Epoch 2/5\n",
      "64/64 - 57s - loss: 2.9354 - lwlrap: 0.3187 - auc: 0.6516 - accuracy: 0.1875 - val_loss: 2.8879 - val_lwlrap: 0.4083 - val_auc: 0.8306 - val_accuracy: 0.2263\n",
      "Epoch 3/5\n",
      "64/64 - 59s - loss: 2.8939 - lwlrap: 0.3350 - auc: 0.6646 - accuracy: 0.2021 - val_loss: 2.8505 - val_lwlrap: 0.4442 - val_auc: 0.8394 - val_accuracy: 0.2675\n",
      "Epoch 4/5\n",
      "64/64 - 56s - loss: 2.8779 - lwlrap: 0.3468 - auc: 0.6611 - accuracy: 0.2334 - val_loss: 2.8194 - val_lwlrap: 0.4404 - val_auc: 0.8415 - val_accuracy: 0.2428\n",
      "Epoch 5/5\n",
      "64/64 - 56s - loss: 2.8683 - lwlrap: 0.3460 - auc: 0.6634 - accuracy: 0.2344 - val_loss: 2.8031 - val_lwlrap: 0.4586 - val_auc: 0.8443 - val_accuracy: 0.2757\n",
      "0.4586372971534729 0.8442813754081726 0.27572017908096313\n",
      "** [1] Train with full model\n",
      "Epoch 1/100\n",
      "64/64 - 60s - loss: 2.8813 - lwlrap: 0.3423 - auc: 0.6646 - accuracy: 0.2222 - val_loss: 2.8914 - val_lwlrap: 0.3609 - val_auc: 0.7971 - val_accuracy: 0.1934\n",
      "Epoch 2/100\n",
      "64/64 - 57s - loss: 2.8064 - lwlrap: 0.3857 - auc: 0.6873 - accuracy: 0.3013 - val_loss: 2.6641 - val_lwlrap: 0.5459 - val_auc: 0.8983 - val_accuracy: 0.3292\n",
      "Epoch 3/100\n",
      "64/64 - 57s - loss: 2.7537 - lwlrap: 0.4142 - auc: 0.7073 - accuracy: 0.3687 - val_loss: 2.6375 - val_lwlrap: 0.5887 - val_auc: 0.9057 - val_accuracy: 0.4033\n",
      "Epoch 4/100\n",
      "64/64 - 57s - loss: 2.7283 - lwlrap: 0.4366 - auc: 0.7232 - accuracy: 0.3843 - val_loss: 2.6093 - val_lwlrap: 0.6569 - val_auc: 0.9185 - val_accuracy: 0.5021\n",
      "Epoch 5/100\n",
      "64/64 - 56s - loss: 2.6947 - lwlrap: 0.4598 - auc: 0.7292 - accuracy: 0.4390 - val_loss: 2.5810 - val_lwlrap: 0.6362 - val_auc: 0.9266 - val_accuracy: 0.4609\n",
      "Epoch 6/100\n",
      "64/64 - 57s - loss: 2.6772 - lwlrap: 0.4731 - auc: 0.7335 - accuracy: 0.4692 - val_loss: 2.5819 - val_lwlrap: 0.6752 - val_auc: 0.9283 - val_accuracy: 0.5021\n",
      "Epoch 7/100\n",
      "64/64 - 57s - loss: 2.6727 - lwlrap: 0.4712 - auc: 0.7271 - accuracy: 0.4697 - val_loss: 2.5729 - val_lwlrap: 0.6866 - val_auc: 0.9323 - val_accuracy: 0.5267\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-58697b4f5e2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'./weights_{fold}_pretrained.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfreeze_backbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_suffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'_{fold}_final'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mval_lwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_lwlrap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-b55ed6c35be1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataset, val_dataset, n_epochs, weights_suffix, with_callbacks)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         )\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=cfg['training']['n_folds'])\n",
    "random_seeds = [398, 182, 718, 580, 403]\n",
    "val_lwraps = []\n",
    "histories = []\n",
    "print('Configs:', cfg)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(samples, targets)):\n",
    "    K.clear_session()\n",
    "    set_random_seeds(random_seeds[fold])\n",
    "\n",
    "    train_dataset = build_dataset(train_idx, samples, is_train=True)\n",
    "    val_dataset = build_dataset(val_idx, samples, is_train=False)\n",
    "\n",
    "    model = ModelNN()\n",
    "    \n",
    "    print(f'* [{fold}] Train with frozen backbone')\n",
    "    history = model.fit(train_dataset, val_dataset, n_epochs=5, weights_suffix=f'_{fold}_pretrained')\n",
    "    print_val_metrics(history)\n",
    "    \n",
    "    print(f'** [{fold}] Train with full model')\n",
    "    model.load_weights(f'./weights_{fold}_pretrained.h5')\n",
    "    model.unfreeze_backbone()\n",
    "    history = model.fit(train_dataset, val_dataset, n_epochs=100, weights_suffix=f'_{fold}_final')\n",
    "\n",
    "    val_lwrap = max(history.history['val_lwlrap'])\n",
    "    val_lwraps.append(val_lwrap)\n",
    "    histories.append(history)\n",
    "    print_val_metrics(history)\n",
    "    \n",
    "print('*** Avg val lwrap:', np.mean(val_lwraps), 'Std:', np.std(val_lwraps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip weights.zip ./weights_*_final.h5\n",
    "!gsutil cp ./weights.zip gs://tn-kaggle-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history, ax=None):\n",
    "    lwlrap = history.history['lwlrap']\n",
    "    val_lwlrap = history.history['val_lwlrap']\n",
    "    df = pd.DataFrame({'train': lwlrap, 'val': val_lwlrap})\n",
    "    df.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(25, 4))\n",
    "for i, history in enumerate(histories):\n",
    "    ax = plt.subplot(1, len(histories), i + 1)\n",
    "    plot_history(history, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN_INFERENCE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_audio(recording_id, train=True):\n",
    "    filepath = os.path.join(TRAIN_INPUT_DIR if train else TEST_INPUT_DIR, recording_id + '.flac')\n",
    "    data, _ = librosa.load(filepath, sr=SR)\n",
    "    return data\n",
    "\n",
    "\n",
    "def cut_audio(audio_data, tmin, tmax, sr=SR, segment_duration=SEGMENT_DURATION):\n",
    "    clip_duration = len(audio_data)/sr\n",
    "    extra_time = max(0, segment_duration - (tmax - tmin)) / 2\n",
    "    tmin = max(0, tmin - extra_time)\n",
    "    tmax = min(clip_duration, tmax + extra_time)\n",
    "\n",
    "    min_sample, max_sample = librosa.time_to_samples([tmin, tmax], sr=sr)\n",
    "    return audio_data[min_sample:(max_sample + 1)]\n",
    "    \n",
    "\n",
    "def get_mel_spec_img(audio_data):\n",
    "    mel_spec = librosa.feature.melspectrogram(audio_data, sr=SR, n_mels=N_MELS)\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "    img = tf.expand_dims(log_mel_spec, -1)\n",
    "    img = tf.image.resize(img, IMG_SIZE[:2])\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    return img, log_mel_spec\n",
    "\n",
    "\n",
    "def cut_and_get_mel_spec_img(audio_data, t_min, t_max):\n",
    "    audio_data = cut_audio(audio_data, t_min, t_max)\n",
    "    return get_mel_spec_img(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(model, recording_id):\n",
    "    audio_data = load_audio(recording_id, train=False)\n",
    "    imgs = []\n",
    "    for t_min in range(0, 55, cfg['inference']['segment_stride']):\n",
    "        t_max = t_min + SEGMENT_DURATION\n",
    "        img, _ = cut_and_get_mel_spec_img(audio_data, t_min, t_max)\n",
    "        img = preprocess_img(img, False)\n",
    "        imgs.append(img)\n",
    "    preds = model.predict(tf.stack(imgs))\n",
    "    return tf.reduce_max(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if RUN_INFERENCE:\n",
    "    model = ModelNN()\n",
    "    model.load_weights('./weights.h5')\n",
    "    preds = predict(model, submission['recording_id'][0])\n",
    "    submission.iloc[0, 1:] = preds.numpy()\n",
    "    submission.to_csv('./submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
