{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.base import BaseEstimator\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: image-classifiers in /opt/conda/lib/python3.7/site-packages (1.0.0)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.7/site-packages (from image-classifiers) (1.0.8)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->image-classifiers) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->image-classifiers) (1.18.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->image-classifiers) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->image-classifiers) (1.18.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install image-classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_models.keras import Classifiers\n",
    "\n",
    "ResNet34, preprocess_resnet34 = Classifiers.get('resnet34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_random_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BASE_INPUT_DIR = './input/'\n",
    "TRAIN_INPUT_DIR = os.path.join(BASE_INPUT_DIR, 'melspec-img')\n",
    "TEST_INPUT_DIR = os.path.join(BASE_INPUT_DIR, 'rfcx-species-audio-detection/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_tp = pd.read_csv(os.path.join(BASE_INPUT_DIR, 'rfcx-species-audio-detection/train_tp.csv'))\n",
    "submission = pd.read_csv(os.path.join(BASE_INPUT_DIR, 'rfcx-species-audio-detection/sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Metrics & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _one_sample_positive_class_precisions(example):\n",
    "    y_true, y_pred = example\n",
    "    y_true = tf.cast(y_true > 0, dtype=tf.int32)\n",
    "\n",
    "    retrieved_classes = tf.argsort(y_pred, direction='DESCENDING')\n",
    "    class_rankings = tf.argsort(retrieved_classes)\n",
    "    retrieved_class_true = tf.gather(y_true, retrieved_classes)\n",
    "    retrieved_cumulative_hits = tf.math.cumsum(tf.cast(retrieved_class_true, tf.float32))\n",
    "\n",
    "    idx = tf.where(y_true)[:, 0]\n",
    "    i = tf.boolean_mask(class_rankings, y_true)\n",
    "    r = tf.gather(retrieved_cumulative_hits, i)\n",
    "    c = 1 + tf.cast(i, tf.float32)\n",
    "    precisions = r / c\n",
    "\n",
    "    dense = tf.scatter_nd(idx[:, None], precisions, [y_pred.shape[0]])\n",
    "    return dense\n",
    "\n",
    "\n",
    "class LWLRAP(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, num_classes, name='lwlrap'):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self._precisions = self.add_weight(\n",
    "            name='per_class_cumulative_precision',\n",
    "            shape=[num_classes],\n",
    "            initializer='zeros',\n",
    "        )\n",
    "\n",
    "        self._counts = self.add_weight(\n",
    "            name='per_class_cumulative_count',\n",
    "            shape=[num_classes],\n",
    "            initializer='zeros',\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        precisions = tf.map_fn(\n",
    "            fn=_one_sample_positive_class_precisions,\n",
    "            elems=(y_true, y_pred),\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "\n",
    "        increments = tf.cast(precisions > 0, tf.float32)\n",
    "        total_increments = tf.reduce_sum(increments, axis=0)\n",
    "        total_precisions = tf.reduce_sum(precisions, axis=0)\n",
    "\n",
    "        self._precisions.assign_add(total_precisions)\n",
    "        self._counts.assign_add(total_increments)        \n",
    "\n",
    "    def result(self):\n",
    "        per_class_lwlrap = self._precisions / tf.maximum(self._counts, 1.0)\n",
    "        per_class_weight = self._counts / tf.reduce_sum(self._counts)\n",
    "        overall_lwlrap = tf.reduce_sum(per_class_lwlrap * per_class_weight)\n",
    "        return overall_lwlrap\n",
    "\n",
    "    def reset_states(self):\n",
    "        self._precisions.assign(self._precisions * 0)\n",
    "        self._counts.assign(self._counts * 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lsep_loss(num_classes, weights_list=None):\n",
    "    if weights_list is None:\n",
    "        weights_list = np.ones(num_classes, dtype=np.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def lsep_loss(y_true, y_pred):\n",
    "        batch_size = tf.math.floordiv(K.sum(K.exp(y_true - y_true)), num_classes)\n",
    "        \n",
    "        y_t = K.reshape(y_true, (batch_size, num_classes))\n",
    "        y_p = K.reshape(y_pred, (batch_size, num_classes))\n",
    "        M_unit = tf.ones((batch_size, num_classes)) \n",
    "        M1 = (M_unit - y_t) * K.reshape(K.tile(weights_list, [batch_size]), (batch_size, num_classes))\n",
    "       \n",
    "        M_pairwise = tf.einsum('ij,ik->ijk', M1, y_t)    # shape = (batch_size, num_classes, num_classes)\n",
    "        M_large = tf.einsum('ij,ik->ijk', M_unit, y_p)  # shape = (batch_size, num_classes, num_classes)\n",
    "\n",
    "        M_diff = K.exp(K.permute_dimensions(M_large, (0, 2, 1)) - M_large)  # shape = (batch_size, num_classes, num_classes)\n",
    "        M = M_pairwise * M_diff  # shape = (batch_size, num_classes, num_classes)\n",
    "\n",
    "        return K.mean(K.log(1 + K.sum(K.sum(M, 2), 1)))\n",
    "    \n",
    "    return lsep_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants & Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_SPECIES = 24\n",
    "\n",
    "IMG_SIZE = (224, 512, 3)\n",
    "IMG_HEIGHT = IMG_SIZE[0]\n",
    "IMG_WIDTH = IMG_SIZE[1]\n",
    "\n",
    "FMIN = 40.0\n",
    "FMAX = 24000.0\n",
    "\n",
    "SR = 48000\n",
    "N_MELS = 224\n",
    "N_FRAMES = 559\n",
    "\n",
    "CLIP_DURATION = 60\n",
    "SEGMENT_DURATION = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "cfg = {\n",
    "    'preprocess': {\n",
    "        'frame_size': 2048,\n",
    "        'hop_length': 512,\n",
    "        'sub_segment_duration': 6,\n",
    "\n",
    "        'spec_aug_prob': 0.7,\n",
    "        'spec_aug_erase_time': 50,\n",
    "        'spec_aug_erase_mel': 16,\n",
    "        'spec_aug_num_time_cuts': 2,\n",
    "        'spec_aug_num_freq_cuts': 4,\n",
    "        \n",
    "        'gauss_noise_prob': 0.7,\n",
    "        'gauss_noise_std': 0.5,\n",
    "        \n",
    "        'random_brightness': 0.3,\n",
    "        \n",
    "        'do_mixup': True,\n",
    "        'mixup_alpha': 0.2,\n",
    "    },\n",
    "    'training': {\n",
    "        'use_tpu': False,\n",
    "        'n_folds': 5,\n",
    "        'batch_size': 32,\n",
    "        'shuffle_buffer_size': 2000,\n",
    "        'steps_per_epoch': 64,\n",
    "\n",
    "        'frozen_learning_rate': 1e-2,\n",
    "        'num_unfreeze_layers': None,\n",
    "\n",
    "        'learning_rate': 3e-4,\n",
    "        'min_lr': 1e-7,\n",
    "    },\n",
    "    'model': {\n",
    "        'backbone_arch': 'effnet_b0',  # effnet_b0, effnet_b1, resnet50, resnet34\n",
    "        'backbone_preprocess': keras.applications.efficientnet.preprocess_input,\n",
    "    },\n",
    "    'inference': {\n",
    "        'segment_stride': 6\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tpu_strategy = None\n",
    "\n",
    "if cfg['training']['use_tpu']:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "    \n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'idx': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'audio_wav': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'recording_id': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'cut_tmin': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'windows': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'species': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "}\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _parse_input_tfrec(example_proto):\n",
    "    sample = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    return {\n",
    "        'idx': tf.io.parse_tensor(sample['idx'], tf.int32),\n",
    "        'audio_wav': tf.io.parse_tensor(sample['audio_wav'], tf.float32),\n",
    "        'recording_id': tf.io.parse_tensor(sample['recording_id'], tf.string),\n",
    "        'cut_tmin': tf.io.parse_tensor(sample['cut_tmin'], tf.float32),\n",
    "        'windows': tf.io.parse_tensor(sample['windows'], tf.float32),\n",
    "        'species': tf.io.parse_tensor(sample['species'], tf.int32),\n",
    "    }\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _cut_wav(x, samples, is_train):\n",
    "    idx = x['idx']\n",
    "    cut_tmin = x['cut_tmin']\n",
    "    main_spid, main_tmin, main_tmax = samples[idx, 0], samples[idx, 1], samples[idx, 2]\n",
    "    main_spid = tf.cast(main_spid, tf.int32)\n",
    "    main_tmin = tf.cast(main_tmin, tf.float32)\n",
    "    main_tmax = tf.cast(main_tmax, tf.float32)\n",
    "    main_tmin -= cut_tmin\n",
    "    main_tmax -= cut_tmin\n",
    "    sub_segment_duration = cfg['preprocess']['sub_segment_duration']\n",
    "    \n",
    "    if is_train:\n",
    "        if main_tmax - main_tmin < sub_segment_duration:\n",
    "            min_left = tf.maximum(0.0, main_tmax - sub_segment_duration)\n",
    "            max_left = tf.minimum(main_tmin, SEGMENT_DURATION - sub_segment_duration)\n",
    "        else:\n",
    "            shrinkage = (main_tmax - main_tmin) - sub_segment_duration\n",
    "            min_left = main_tmin\n",
    "            max_left = main_tmin + shrinkage\n",
    "        left_cut = tf.random.uniform([], minval=min_left, maxval=max_left)\n",
    "    else:\n",
    "        if main_tmax - main_tmin < sub_segment_duration:\n",
    "            extension = tf.maximum(0.0, sub_segment_duration - (main_tmax - main_tmin))/2\n",
    "            left_extend = extension\n",
    "            if main_tmax + extension > SEGMENT_DURATION:\n",
    "                left_extend += main_tmax + extension - SEGMENT_DURATION\n",
    "            left_cut = tf.maximum(0.0, main_tmin - left_extend)\n",
    "        else:\n",
    "            shrinkage = (main_tmax - main_tmin) - sub_segment_duration\n",
    "            left_cut = main_tmin + shrinkage/2\n",
    "    \n",
    "    left_cut_sample = tf.cast(tf.floor(left_cut * SR), tf.int32)\n",
    "    right_cut_sample = left_cut_sample + sub_segment_duration*SR\n",
    "    x = x.copy()\n",
    "    x['audio_wav'] = tf.reshape(x['audio_wav'][left_cut_sample:right_cut_sample], [sub_segment_duration*SR])\n",
    "    x['windows'] = [[main_tmin, main_tmax]]\n",
    "    x['species'] = [main_spid]\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _wav_to_mel_spec(x):\n",
    "    frame_size = cfg['preprocess']['frame_size']\n",
    "    hop_length = cfg['preprocess']['hop_length']\n",
    "    \n",
    "    stfts = tf.signal.stft(\n",
    "        x[\"audio_wav\"],\n",
    "        frame_length=frame_size,\n",
    "        frame_step=hop_length,\n",
    "        fft_length=frame_size,\n",
    "        window_fn=tf.signal.hann_window,\n",
    "    )\n",
    "    hz_spec = tf.square(tf.abs(stfts))\n",
    "\n",
    "    n_hz_spec_bins = tf.shape(stfts)[-1]\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(N_MELS, n_hz_spec_bins, SR, FMIN, FMAX)\n",
    "    mel_spec = tf.tensordot(hz_spec, linear_to_mel_weight_matrix, 1)\n",
    "    mel_spec.set_shape(hz_spec.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
    "    log_mel_spec = tf.math.log(mel_spec + 1e-6)/tf.math.log(10.0)\n",
    "\n",
    "    y = {\n",
    "        'mel_spec': tf.transpose(log_mel_spec)\n",
    "    }\n",
    "    y.update(x)\n",
    "    return y\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _create_labels(x):\n",
    "    species, _ = tf.unique(tf.cast(x['species'], dtype=tf.int64))\n",
    "    species = tf.sort(species)\n",
    "    n_labels = tf.shape(species)[0]\n",
    "    species = tf.reshape(species, (-1, 1))\n",
    "    return tf.sparse.to_dense(tf.sparse.SparseTensor(species, tf.ones([tf.shape(species)[0]]), [NUM_SPECIES]))\n",
    "\n",
    "\n",
    "def _create_idx_filter(indices):\n",
    "    @tf.function\n",
    "    def _filter_func(x):\n",
    "        return tf.reduce_any(indices == x['idx'])\n",
    "    return _filter_func\n",
    "\n",
    "\n",
    "def _filter_indices(dataset, indices):\n",
    "    return dataset.filter(_create_idx_filter(indices))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def augment_img(image):\n",
    "    \n",
    "    @tf.function\n",
    "    def _specaugment(image):\n",
    "        erase_time = cfg['preprocess']['spec_aug_erase_time']\n",
    "        erase_mel = cfg['preprocess']['spec_aug_erase_mel']\n",
    "        num_time_cuts = cfg['preprocess']['spec_aug_num_time_cuts']\n",
    "        num_freq_cuts = cfg['preprocess']['spec_aug_num_freq_cuts']\n",
    "        img_height = tf.shape(image)[0]\n",
    "        img_width = tf.shape(image)[1]\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "\n",
    "        xoff = tf.random.uniform([num_time_cuts], minval=erase_time//2, maxval=img_width - erase_time//2, dtype=tf.int32)\n",
    "        xsize = tf.random.uniform([num_time_cuts], minval=erase_time//2, maxval=erase_time, dtype=tf.int32)\n",
    "        yoff = tf.random.uniform([num_freq_cuts], minval=erase_mel//2, maxval=img_height-erase_mel//2, dtype=tf.int32)\n",
    "        ysize = tf.random.uniform([num_freq_cuts], minval=erase_mel//2, maxval=erase_mel, dtype=tf.int32)\n",
    "\n",
    "        for i in range(num_time_cuts):\n",
    "            image = tfa.image.cutout(image, [img_height, xsize[i]], offset=[img_height//2, xoff[i]])\n",
    "        for i in range(num_freq_cuts):\n",
    "            image = tfa.image.cutout(image, [ysize[i], img_width], offset=[yoff[i], img_width//2])\n",
    "\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "        return image\n",
    "    \n",
    "    gauss_noise = tf.keras.layers.GaussianNoise(cfg['preprocess']['gauss_noise_std']) \n",
    "    image = tf.cond(\n",
    "        tf.random.uniform([]) < cfg['preprocess']['gauss_noise_prob'],\n",
    "        lambda: gauss_noise(image, training=True), lambda: image\n",
    "    )\n",
    "    image = tf.image.random_brightness(image, cfg['preprocess']['random_brightness'])\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.cond(\n",
    "        tf.random.uniform([]) < cfg['preprocess']['spec_aug_prob'],\n",
    "        lambda: _specaugment(image), lambda: image\n",
    "    )\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _mixup(inp, targ):\n",
    "    indices = tf.range(len(inp))\n",
    "    indices = tf.random.shuffle(indices)\n",
    "    sinp = tf.gather(inp, indices, axis=0)\n",
    "    starg = tf.gather(targ, indices, axis=0)\n",
    "\n",
    "    alpha = cfg['preprocess']['mixup_alpha']\n",
    "    t = tf.compat.v1.distributions.Beta(alpha, alpha).sample([len(inp)])\n",
    "    tx = tf.reshape(t, [-1, 1, 1, 1])\n",
    "    ty = tf.reshape(t, [-1, 1])\n",
    "    x = inp * tx + sinp * (1 - tx)\n",
    "    y = targ * ty + starg * (1 - ty)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def preprocess_img(img, is_train):\n",
    "    img.set_shape([N_MELS, N_FRAMES])\n",
    "    img = tf.expand_dims(img, axis=-1)\n",
    "    img = tf.image.resize(img, IMG_SIZE[:-1])\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    if is_train:\n",
    "        img = augment_img(img)\n",
    "    img_min = tf.reduce_min(img)\n",
    "    img_max = tf.reduce_max(img)\n",
    "    img = (img - img_min)/(img_max - img_min)*255\n",
    "    img = tf.image.grayscale_to_rgb(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def build_dataset(indices, samples, is_train):\n",
    "    \n",
    "    @tf.function\n",
    "    def _split_img_label(x):\n",
    "        return x['mel_spec'], _create_labels(x)\n",
    "    \n",
    "    @tf.function\n",
    "    def _cut_sub_segment(x):\n",
    "        return _cut_wav(x, samples, is_train)\n",
    "    \n",
    "    samples = samples[['species_id', 't_min', 't_max']].values\n",
    "    filepath = os.path.join(TRAIN_INPUT_DIR, 'train.cut_audio.tfrecord')\n",
    "    ds = tf.data.TFRecordDataset(filepath)\n",
    "    ds = ds.map(_parse_input_tfrec, num_parallel_calls=AUTOTUNE)\n",
    "    ds = _filter_indices(ds, indices)\n",
    "    ds = ds.cache()\n",
    "    if is_train:\n",
    "        ds = ds.shuffle(buffer_size=cfg['training']['shuffle_buffer_size'])\n",
    "        ds = ds.repeat()\n",
    "    ds = ds.map(_cut_sub_segment, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(_wav_to_mel_spec, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(_split_img_label, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(lambda img, l: (preprocess_img(img, is_train), l), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(cfg['training']['batch_size'])\n",
    "    if is_train and cfg['preprocess']['do_mixup']:\n",
    "        ds = ds.map(_mixup, num_parallel_calls=AUTOTUNE)\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AttBlock(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_outputs, activation='linear', temperature=1.):\n",
    "        super(AttBlock, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.activation = activation\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.att = keras.layers.Conv1D(self.num_outputs, 1)\n",
    "        self.cla = keras.layers.Conv1D(self.num_outputs, 1)\n",
    "        self.bn_att = keras.layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, x):\n",
    "        norm_att = tf.nn.softmax(tf.clip_by_value(self.att(x), -10, 10), axis=1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = tf.math.reduce_sum(norm_att * cla, axis=1)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return tf.math.sigmoid(x)\n",
    "        \n",
    "        \n",
    "class ModelNN(BaseEstimator):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes=NUM_SPECIES,\n",
    "        backbone_arch=cfg['model']['backbone_arch'],\n",
    "    ):\n",
    "        self.n_classes = n_classes\n",
    "        self.backbone_arch = backbone_arch\n",
    "        self.estimator_ = None\n",
    "        \n",
    "    def compile_model(self, model, is_frozen):\n",
    "        if is_frozen:\n",
    "            learning_rate = cfg['training']['frozen_learning_rate']\n",
    "        else:\n",
    "            learning_rate = cfg['training']['learning_rate']\n",
    "            \n",
    "        lr_schedule = tf.keras.experimental.CosineDecayRestarts(\n",
    "            learning_rate, 1000, t_mul=2.0, m_mul=1.0, alpha=1e-8, name='consine_decay'\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "            loss='bce',\n",
    "            loss_weights=[1, 0.5],\n",
    "            metrics=[[LWLRAP(self.n_classes), 'accuracy'], [LWLRAP(self.n_classes), 'accuracy']]\n",
    "        )\n",
    "        return model\n",
    "        \n",
    "    def build_model(self, freeze_backbone):\n",
    "        inputs = keras.layers.Input(shape=IMG_SIZE)\n",
    "\n",
    "        if self.backbone_arch == 'effnet_b0':\n",
    "            model_backbone = keras.applications.EfficientNetB0(include_top=False, input_tensor=inputs)\n",
    "        elif self.backbone_arch == 'effnet_b1':\n",
    "            model_backbone = keras.applications.EfficientNetB1(include_top=False, input_tensor=inputs)\n",
    "        elif self.backbone_arch == 'resnet50':\n",
    "            model_backbone = keras.applications.ResNet50(include_top=False, input_tensor=inputs)\n",
    "        elif self.backbone_arch == 'resnet34':\n",
    "            model_backbone = ResNet34(include_top=False, input_tensor=inputs, weights='imagenet')\n",
    "        else:\n",
    "            raise Exception(f'Unsupported backbone arch: {self.backbone_arch}')\n",
    "            \n",
    "        if freeze_backbone:\n",
    "            model_backbone.trainable = False\n",
    "        \n",
    "        x = tf.math.reduce_mean(model_backbone.output, axis=1)\n",
    "        x1 = keras.layers.MaxPool1D(pool_size=3, strides=1, padding='same')(x)\n",
    "        x2 = keras.layers.AveragePooling1D(pool_size=3, strides=1, padding='same')(x)\n",
    "        x = x1 + x2\n",
    "        x = keras.layers.Dropout(0.5)(x)\n",
    "        x = keras.layers.TimeDistributed(keras.layers.Dense(2048, activation='relu'))(x)\n",
    "        x = keras.layers.Dropout(0.5)(x)\n",
    "        (clipwise_output, _, segmentwise_output) = AttBlock(self.n_classes)(x)\n",
    "        segmentwise_output = tf.math.reduce_max(segmentwise_output, axis=1)\n",
    "        outputs = [\n",
    "            keras.layers.Activation('sigmoid', name='clipwise')(clipwise_output),\n",
    "            keras.layers.Activation('sigmoid', name='segmentwise')(segmentwise_output)\n",
    "        ]\n",
    "\n",
    "        model = keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
    "        return self.compile_model(model, freeze_backbone)\n",
    "    \n",
    "    def unfreeze_backbone(self, num_layers=cfg['training']['num_unfreeze_layers']):\n",
    "        model = self.estimator_\n",
    "        selected_layers = model.layers[-num_layers:] if num_layers is not None else model.layers\n",
    "        for layer in selected_layers:\n",
    "            if not isinstance(layer, keras.layers.BatchNormalization):\n",
    "                layer.trainable = True\n",
    "        self.estimator_ = self.compile_model(model, False)\n",
    "  \n",
    "    def fit(self, train_dataset, val_dataset, n_epochs=25, weights_suffix='', with_callbacks=True):\n",
    "        if self.estimator_ is None:\n",
    "            if cfg['training']['use_tpu']:\n",
    "                with tpu_strategy.scope():\n",
    "                    self.estimator_ = self.build_model(freeze_backbone=True)\n",
    "            else:\n",
    "                self.estimator_ = self.build_model(freeze_backbone=True)\n",
    "        \n",
    "        model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "            f'./weights{weights_suffix}.h5',\n",
    "            monitor='val_clipwise_lwlrap',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode='max'\n",
    "        )\n",
    "        lr_scheduler = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_clipwise_lwlrap',\n",
    "            patience=5,\n",
    "            mode='max',\n",
    "            min_lr=cfg['training']['min_lr'],\n",
    "            verbose=2\n",
    "        )\n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_clipwise_lwlrap',\n",
    "            patience=15,\n",
    "            mode='max',\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        logs_cb = tf.keras.callbacks.CSVLogger(f'logs/metrics{weights_suffix}.csv')\n",
    "#         callbacks = [model_checkpoint, lr_scheduler, early_stopping, logs_cb]\n",
    "        callbacks = [model_checkpoint, early_stopping, logs_cb]\n",
    "\n",
    "        return self.estimator_.fit(\n",
    "            train_dataset,\n",
    "            epochs=n_epochs,\n",
    "            callbacks=callbacks if with_callbacks else None,\n",
    "            validation_data=val_dataset,\n",
    "            steps_per_epoch=cfg['training']['steps_per_epoch'],\n",
    "            verbose=2\n",
    "        )\n",
    "    \n",
    "    def predict(self, img):\n",
    "        return self.estimator_.predict(img)\n",
    "    \n",
    "    def load_weights(self, checkpoint_path):\n",
    "        if self.estimator_ is None:\n",
    "            self.estimator_ = self.build_model(freeze_backbone=False)\n",
    "            self.unfreeze_backbone()\n",
    "        self.estimator_.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_val_metrics(history):\n",
    "    max_idx = np.argmax(history.history['val_clipwise_lwlrap'])\n",
    "    print(\n",
    "        history.history['val_clipwise_lwlrap'][max_idx],\n",
    "        history.history['val_segmentwise_lwlrap'][max_idx],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = train_tp.copy()\n",
    "targets = train_tp['species_id'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs: {'preprocess': {'frame_size': 2048, 'hop_length': 512, 'sub_segment_duration': 6, 'spec_aug_prob': 0.7, 'spec_aug_erase_time': 50, 'spec_aug_erase_mel': 16, 'spec_aug_num_time_cuts': 2, 'spec_aug_num_freq_cuts': 4, 'gauss_noise_prob': 0.7, 'gauss_noise_std': 0.5, 'random_brightness': 0.3, 'do_mixup': True, 'mixup_alpha': 0.2}, 'training': {'use_tpu': False, 'n_folds': 5, 'batch_size': 32, 'shuffle_buffer_size': 2000, 'steps_per_epoch': 64, 'frozen_learning_rate': 0.01, 'num_unfreeze_layers': None, 'learning_rate': 0.0003, 'min_lr': 1e-07}, 'model': {'backbone_arch': 'effnet_b0', 'backbone_preprocess': <function preprocess_input at 0x7f18decfdd40>}, 'inference': {'segment_stride': 6}}\n",
      "* [0] Train with frozen backbone\n",
      "Epoch 1/5\n",
      "64/64 - 50s - loss: 0.8332 - clipwise_loss: 0.5662 - segmentwise_loss: 0.5341 - clipwise_lwlrap: 0.2141 - clipwise_accuracy: 0.0630 - segmentwise_lwlrap: 0.2199 - segmentwise_accuracy: 0.0684 - val_loss: 0.2503 - val_clipwise_loss: 0.1675 - val_segmentwise_loss: 0.1657 - val_clipwise_lwlrap: 0.2652 - val_clipwise_accuracy: 0.1107 - val_segmentwise_lwlrap: 0.2589 - val_segmentwise_accuracy: 0.1025\n",
      "Epoch 2/5\n",
      "64/64 - 48s - loss: 0.2790 - clipwise_loss: 0.1804 - segmentwise_loss: 0.1973 - clipwise_lwlrap: 0.2498 - clipwise_accuracy: 0.0991 - segmentwise_lwlrap: 0.2459 - segmentwise_accuracy: 0.0967 - val_loss: 0.2427 - val_clipwise_loss: 0.1628 - val_segmentwise_loss: 0.1597 - val_clipwise_lwlrap: 0.3128 - val_clipwise_accuracy: 0.1475 - val_segmentwise_lwlrap: 0.2911 - val_segmentwise_accuracy: 0.1270\n",
      "Epoch 3/5\n",
      "64/64 - 48s - loss: 0.2769 - clipwise_loss: 0.1794 - segmentwise_loss: 0.1950 - clipwise_lwlrap: 0.2586 - clipwise_accuracy: 0.1006 - segmentwise_lwlrap: 0.2536 - segmentwise_accuracy: 0.0957 - val_loss: 0.2488 - val_clipwise_loss: 0.1687 - val_segmentwise_loss: 0.1601 - val_clipwise_lwlrap: 0.2997 - val_clipwise_accuracy: 0.1434 - val_segmentwise_lwlrap: 0.3106 - val_segmentwise_accuracy: 0.1516\n",
      "Epoch 4/5\n",
      "64/64 - 48s - loss: 0.2756 - clipwise_loss: 0.1774 - segmentwise_loss: 0.1963 - clipwise_lwlrap: 0.2590 - clipwise_accuracy: 0.0928 - segmentwise_lwlrap: 0.2535 - segmentwise_accuracy: 0.1045 - val_loss: 0.2505 - val_clipwise_loss: 0.1689 - val_segmentwise_loss: 0.1632 - val_clipwise_lwlrap: 0.3213 - val_clipwise_accuracy: 0.1393 - val_segmentwise_lwlrap: 0.3033 - val_segmentwise_accuracy: 0.1270\n",
      "Epoch 5/5\n",
      "64/64 - 48s - loss: 0.2728 - clipwise_loss: 0.1759 - segmentwise_loss: 0.1939 - clipwise_lwlrap: 0.2664 - clipwise_accuracy: 0.1094 - segmentwise_lwlrap: 0.2643 - segmentwise_accuracy: 0.1221 - val_loss: 0.2481 - val_clipwise_loss: 0.1690 - val_segmentwise_loss: 0.1581 - val_clipwise_lwlrap: 0.3316 - val_clipwise_accuracy: 0.1557 - val_segmentwise_lwlrap: 0.3345 - val_segmentwise_accuracy: 0.1721\n",
      "0.3315575122833252 0.3344825506210327\n",
      "** [0] Train with full model\n",
      "Epoch 1/100\n",
      "64/64 - 59s - loss: 0.2604 - clipwise_loss: 0.1681 - segmentwise_loss: 0.1846 - clipwise_lwlrap: 0.2676 - clipwise_accuracy: 0.0967 - segmentwise_lwlrap: 0.2659 - segmentwise_accuracy: 0.1040 - val_loss: 0.2353 - val_clipwise_loss: 0.1586 - val_segmentwise_loss: 0.1535 - val_clipwise_lwlrap: 0.3376 - val_clipwise_accuracy: 0.1598 - val_segmentwise_lwlrap: 0.3404 - val_segmentwise_accuracy: 0.1639\n",
      "Epoch 2/100\n",
      "64/64 - 57s - loss: 0.2476 - clipwise_loss: 0.1605 - segmentwise_loss: 0.1742 - clipwise_lwlrap: 0.2944 - clipwise_accuracy: 0.1392 - segmentwise_lwlrap: 0.2870 - segmentwise_accuracy: 0.1328 - val_loss: 0.2258 - val_clipwise_loss: 0.1526 - val_segmentwise_loss: 0.1463 - val_clipwise_lwlrap: 0.4011 - val_clipwise_accuracy: 0.2131 - val_segmentwise_lwlrap: 0.3971 - val_segmentwise_accuracy: 0.2131\n",
      "Epoch 3/100\n",
      "64/64 - 58s - loss: 0.2393 - clipwise_loss: 0.1562 - segmentwise_loss: 0.1664 - clipwise_lwlrap: 0.3222 - clipwise_accuracy: 0.1890 - segmentwise_lwlrap: 0.3244 - segmentwise_accuracy: 0.1997 - val_loss: 0.2074 - val_clipwise_loss: 0.1401 - val_segmentwise_loss: 0.1345 - val_clipwise_lwlrap: 0.4631 - val_clipwise_accuracy: 0.2582 - val_segmentwise_lwlrap: 0.4798 - val_segmentwise_accuracy: 0.3074\n",
      "Epoch 4/100\n",
      "64/64 - 57s - loss: 0.2281 - clipwise_loss: 0.1481 - segmentwise_loss: 0.1600 - clipwise_lwlrap: 0.3567 - clipwise_accuracy: 0.2344 - segmentwise_lwlrap: 0.3659 - segmentwise_accuracy: 0.2754 - val_loss: 0.1954 - val_clipwise_loss: 0.1316 - val_segmentwise_loss: 0.1277 - val_clipwise_lwlrap: 0.5082 - val_clipwise_accuracy: 0.3197 - val_segmentwise_lwlrap: 0.5243 - val_segmentwise_accuracy: 0.3484\n",
      "Epoch 5/100\n",
      "64/64 - 57s - loss: 0.2182 - clipwise_loss: 0.1415 - segmentwise_loss: 0.1533 - clipwise_lwlrap: 0.3846 - clipwise_accuracy: 0.3120 - segmentwise_lwlrap: 0.3871 - segmentwise_accuracy: 0.3174 - val_loss: 0.1856 - val_clipwise_loss: 0.1260 - val_segmentwise_loss: 0.1192 - val_clipwise_lwlrap: 0.5453 - val_clipwise_accuracy: 0.3689 - val_segmentwise_lwlrap: 0.5676 - val_segmentwise_accuracy: 0.4016\n",
      "Epoch 6/100\n",
      "64/64 - 56s - loss: 0.2082 - clipwise_loss: 0.1353 - segmentwise_loss: 0.1458 - clipwise_lwlrap: 0.4149 - clipwise_accuracy: 0.3560 - segmentwise_lwlrap: 0.4152 - segmentwise_accuracy: 0.3677 - val_loss: 0.1712 - val_clipwise_loss: 0.1153 - val_segmentwise_loss: 0.1119 - val_clipwise_lwlrap: 0.6165 - val_clipwise_accuracy: 0.4713 - val_segmentwise_lwlrap: 0.6103 - val_segmentwise_accuracy: 0.4672\n",
      "Epoch 7/100\n",
      "64/64 - 57s - loss: 0.1997 - clipwise_loss: 0.1295 - segmentwise_loss: 0.1405 - clipwise_lwlrap: 0.4360 - clipwise_accuracy: 0.4136 - segmentwise_lwlrap: 0.4400 - segmentwise_accuracy: 0.4263 - val_loss: 0.1691 - val_clipwise_loss: 0.1149 - val_segmentwise_loss: 0.1083 - val_clipwise_lwlrap: 0.6426 - val_clipwise_accuracy: 0.4959 - val_segmentwise_lwlrap: 0.6352 - val_segmentwise_accuracy: 0.4877\n",
      "Epoch 8/100\n",
      "64/64 - 57s - loss: 0.1902 - clipwise_loss: 0.1229 - segmentwise_loss: 0.1346 - clipwise_lwlrap: 0.4497 - clipwise_accuracy: 0.4551 - segmentwise_lwlrap: 0.4509 - segmentwise_accuracy: 0.4600 - val_loss: 0.1534 - val_clipwise_loss: 0.1024 - val_segmentwise_loss: 0.1020 - val_clipwise_lwlrap: 0.6926 - val_clipwise_accuracy: 0.5820 - val_segmentwise_lwlrap: 0.6787 - val_segmentwise_accuracy: 0.5615\n",
      "Epoch 9/100\n",
      "64/64 - 57s - loss: 0.1797 - clipwise_loss: 0.1155 - segmentwise_loss: 0.1283 - clipwise_lwlrap: 0.4780 - clipwise_accuracy: 0.5161 - segmentwise_lwlrap: 0.4816 - segmentwise_accuracy: 0.5244 - val_loss: 0.1507 - val_clipwise_loss: 0.0997 - val_segmentwise_loss: 0.1020 - val_clipwise_lwlrap: 0.6891 - val_clipwise_accuracy: 0.5656 - val_segmentwise_lwlrap: 0.6910 - val_segmentwise_accuracy: 0.5656\n",
      "Epoch 10/100\n",
      "64/64 - 57s - loss: 0.1722 - clipwise_loss: 0.1115 - segmentwise_loss: 0.1214 - clipwise_lwlrap: 0.4832 - clipwise_accuracy: 0.5342 - segmentwise_lwlrap: 0.4874 - segmentwise_accuracy: 0.5552 - val_loss: 0.1446 - val_clipwise_loss: 0.0946 - val_segmentwise_loss: 0.1000 - val_clipwise_lwlrap: 0.6944 - val_clipwise_accuracy: 0.5697 - val_segmentwise_lwlrap: 0.6968 - val_segmentwise_accuracy: 0.5820\n",
      "Epoch 11/100\n",
      "64/64 - 57s - loss: 0.1690 - clipwise_loss: 0.1087 - segmentwise_loss: 0.1206 - clipwise_lwlrap: 0.4902 - clipwise_accuracy: 0.5493 - segmentwise_lwlrap: 0.4974 - segmentwise_accuracy: 0.5635 - val_loss: 0.1357 - val_clipwise_loss: 0.0902 - val_segmentwise_loss: 0.0909 - val_clipwise_lwlrap: 0.7181 - val_clipwise_accuracy: 0.5943 - val_segmentwise_lwlrap: 0.7159 - val_segmentwise_accuracy: 0.5984\n",
      "Epoch 12/100\n",
      "64/64 - 57s - loss: 0.1650 - clipwise_loss: 0.1062 - segmentwise_loss: 0.1175 - clipwise_lwlrap: 0.4973 - clipwise_accuracy: 0.5615 - segmentwise_lwlrap: 0.4978 - segmentwise_accuracy: 0.5718 - val_loss: 0.1359 - val_clipwise_loss: 0.0900 - val_segmentwise_loss: 0.0918 - val_clipwise_lwlrap: 0.7196 - val_clipwise_accuracy: 0.6025 - val_segmentwise_lwlrap: 0.7184 - val_segmentwise_accuracy: 0.6025\n",
      "Epoch 13/100\n",
      "64/64 - 56s - loss: 0.1587 - clipwise_loss: 0.1019 - segmentwise_loss: 0.1136 - clipwise_lwlrap: 0.5083 - clipwise_accuracy: 0.5874 - segmentwise_lwlrap: 0.5101 - segmentwise_accuracy: 0.5952 - val_loss: 0.1342 - val_clipwise_loss: 0.0884 - val_segmentwise_loss: 0.0917 - val_clipwise_lwlrap: 0.7255 - val_clipwise_accuracy: 0.6025 - val_segmentwise_lwlrap: 0.7147 - val_segmentwise_accuracy: 0.5861\n",
      "Epoch 14/100\n",
      "64/64 - 57s - loss: 0.1604 - clipwise_loss: 0.1033 - segmentwise_loss: 0.1142 - clipwise_lwlrap: 0.5139 - clipwise_accuracy: 0.5903 - segmentwise_lwlrap: 0.5162 - segmentwise_accuracy: 0.5947 - val_loss: 0.1342 - val_clipwise_loss: 0.0881 - val_segmentwise_loss: 0.0922 - val_clipwise_lwlrap: 0.7261 - val_clipwise_accuracy: 0.5984 - val_segmentwise_lwlrap: 0.7141 - val_segmentwise_accuracy: 0.5861\n",
      "Epoch 15/100\n",
      "64/64 - 56s - loss: 0.1608 - clipwise_loss: 0.1028 - segmentwise_loss: 0.1161 - clipwise_lwlrap: 0.5060 - clipwise_accuracy: 0.5967 - segmentwise_lwlrap: 0.5082 - segmentwise_accuracy: 0.6050 - val_loss: 0.1348 - val_clipwise_loss: 0.0886 - val_segmentwise_loss: 0.0924 - val_clipwise_lwlrap: 0.7217 - val_clipwise_accuracy: 0.5984 - val_segmentwise_lwlrap: 0.7151 - val_segmentwise_accuracy: 0.5902\n",
      "Epoch 16/100\n",
      "64/64 - 56s - loss: 0.1639 - clipwise_loss: 0.1052 - segmentwise_loss: 0.1174 - clipwise_lwlrap: 0.4987 - clipwise_accuracy: 0.5835 - segmentwise_lwlrap: 0.5011 - segmentwise_accuracy: 0.5884 - val_loss: 0.1396 - val_clipwise_loss: 0.0923 - val_segmentwise_loss: 0.0946 - val_clipwise_lwlrap: 0.6997 - val_clipwise_accuracy: 0.5697 - val_segmentwise_lwlrap: 0.6935 - val_segmentwise_accuracy: 0.5574\n",
      "Epoch 17/100\n",
      "64/64 - 57s - loss: 0.1759 - clipwise_loss: 0.1133 - segmentwise_loss: 0.1252 - clipwise_lwlrap: 0.4737 - clipwise_accuracy: 0.5171 - segmentwise_lwlrap: 0.4809 - segmentwise_accuracy: 0.5332 - val_loss: 0.1413 - val_clipwise_loss: 0.0956 - val_segmentwise_loss: 0.0915 - val_clipwise_lwlrap: 0.7323 - val_clipwise_accuracy: 0.6311 - val_segmentwise_lwlrap: 0.7322 - val_segmentwise_accuracy: 0.6270\n",
      "Epoch 18/100\n",
      "64/64 - 57s - loss: 0.1650 - clipwise_loss: 0.1061 - segmentwise_loss: 0.1178 - clipwise_lwlrap: 0.5033 - clipwise_accuracy: 0.5825 - segmentwise_lwlrap: 0.5034 - segmentwise_accuracy: 0.5903 - val_loss: 0.1422 - val_clipwise_loss: 0.0944 - val_segmentwise_loss: 0.0956 - val_clipwise_lwlrap: 0.6872 - val_clipwise_accuracy: 0.5533 - val_segmentwise_lwlrap: 0.7001 - val_segmentwise_accuracy: 0.5738\n",
      "Epoch 19/100\n",
      "64/64 - 57s - loss: 0.1626 - clipwise_loss: 0.1046 - segmentwise_loss: 0.1159 - clipwise_lwlrap: 0.5065 - clipwise_accuracy: 0.5845 - segmentwise_lwlrap: 0.5097 - segmentwise_accuracy: 0.5884 - val_loss: 0.1261 - val_clipwise_loss: 0.0837 - val_segmentwise_loss: 0.0850 - val_clipwise_lwlrap: 0.7412 - val_clipwise_accuracy: 0.6270 - val_segmentwise_lwlrap: 0.7462 - val_segmentwise_accuracy: 0.6393\n",
      "Epoch 20/100\n",
      "64/64 - 56s - loss: 0.1577 - clipwise_loss: 0.1014 - segmentwise_loss: 0.1127 - clipwise_lwlrap: 0.5096 - clipwise_accuracy: 0.5962 - segmentwise_lwlrap: 0.5161 - segmentwise_accuracy: 0.6147 - val_loss: 0.1269 - val_clipwise_loss: 0.0833 - val_segmentwise_loss: 0.0872 - val_clipwise_lwlrap: 0.7431 - val_clipwise_accuracy: 0.6189 - val_segmentwise_lwlrap: 0.7386 - val_segmentwise_accuracy: 0.6066\n",
      "Epoch 21/100\n",
      "64/64 - 57s - loss: 0.1495 - clipwise_loss: 0.0959 - segmentwise_loss: 0.1071 - clipwise_lwlrap: 0.5221 - clipwise_accuracy: 0.6318 - segmentwise_lwlrap: 0.5227 - segmentwise_accuracy: 0.6260 - val_loss: 0.1252 - val_clipwise_loss: 0.0835 - val_segmentwise_loss: 0.0833 - val_clipwise_lwlrap: 0.7728 - val_clipwise_accuracy: 0.6721 - val_segmentwise_lwlrap: 0.7623 - val_segmentwise_accuracy: 0.6516\n",
      "Epoch 22/100\n",
      "64/64 - 57s - loss: 0.1465 - clipwise_loss: 0.0946 - segmentwise_loss: 0.1037 - clipwise_lwlrap: 0.5252 - clipwise_accuracy: 0.6328 - segmentwise_lwlrap: 0.5295 - segmentwise_accuracy: 0.6509 - val_loss: 0.1153 - val_clipwise_loss: 0.0778 - val_segmentwise_loss: 0.0749 - val_clipwise_lwlrap: 0.7960 - val_clipwise_accuracy: 0.6926 - val_segmentwise_lwlrap: 0.8036 - val_segmentwise_accuracy: 0.7049\n",
      "Epoch 23/100\n",
      "64/64 - 57s - loss: 0.1436 - clipwise_loss: 0.0929 - segmentwise_loss: 0.1013 - clipwise_lwlrap: 0.5339 - clipwise_accuracy: 0.6562 - segmentwise_lwlrap: 0.5401 - segmentwise_accuracy: 0.6646 - val_loss: 0.1201 - val_clipwise_loss: 0.0792 - val_segmentwise_loss: 0.0819 - val_clipwise_lwlrap: 0.7591 - val_clipwise_accuracy: 0.6516 - val_segmentwise_lwlrap: 0.7633 - val_segmentwise_accuracy: 0.6557\n",
      "Epoch 24/100\n",
      "64/64 - 57s - loss: 0.1349 - clipwise_loss: 0.0867 - segmentwise_loss: 0.0963 - clipwise_lwlrap: 0.5459 - clipwise_accuracy: 0.6943 - segmentwise_lwlrap: 0.5474 - segmentwise_accuracy: 0.6890 - val_loss: 0.1161 - val_clipwise_loss: 0.0763 - val_segmentwise_loss: 0.0796 - val_clipwise_lwlrap: 0.7817 - val_clipwise_accuracy: 0.6762 - val_segmentwise_lwlrap: 0.7899 - val_segmentwise_accuracy: 0.6885\n",
      "Epoch 25/100\n",
      "64/64 - 57s - loss: 0.1297 - clipwise_loss: 0.0835 - segmentwise_loss: 0.0924 - clipwise_lwlrap: 0.5602 - clipwise_accuracy: 0.7070 - segmentwise_lwlrap: 0.5633 - segmentwise_accuracy: 0.7109 - val_loss: 0.1115 - val_clipwise_loss: 0.0750 - val_segmentwise_loss: 0.0732 - val_clipwise_lwlrap: 0.8050 - val_clipwise_accuracy: 0.7008 - val_segmentwise_lwlrap: 0.8107 - val_segmentwise_accuracy: 0.7131\n",
      "Epoch 26/100\n",
      "64/64 - 57s - loss: 0.1331 - clipwise_loss: 0.0861 - segmentwise_loss: 0.0940 - clipwise_lwlrap: 0.5484 - clipwise_accuracy: 0.7046 - segmentwise_lwlrap: 0.5574 - segmentwise_accuracy: 0.7075 - val_loss: 0.1074 - val_clipwise_loss: 0.0707 - val_segmentwise_loss: 0.0734 - val_clipwise_lwlrap: 0.8106 - val_clipwise_accuracy: 0.7131 - val_segmentwise_lwlrap: 0.8101 - val_segmentwise_accuracy: 0.7090\n",
      "Epoch 27/100\n",
      "64/64 - 56s - loss: 0.1277 - clipwise_loss: 0.0823 - segmentwise_loss: 0.0906 - clipwise_lwlrap: 0.5573 - clipwise_accuracy: 0.7241 - segmentwise_lwlrap: 0.5595 - segmentwise_accuracy: 0.7300 - val_loss: 0.1042 - val_clipwise_loss: 0.0691 - val_segmentwise_loss: 0.0702 - val_clipwise_lwlrap: 0.8114 - val_clipwise_accuracy: 0.7090 - val_segmentwise_lwlrap: 0.8019 - val_segmentwise_accuracy: 0.6926\n",
      "Epoch 28/100\n",
      "64/64 - 57s - loss: 0.1247 - clipwise_loss: 0.0802 - segmentwise_loss: 0.0891 - clipwise_lwlrap: 0.5682 - clipwise_accuracy: 0.7368 - segmentwise_lwlrap: 0.5702 - segmentwise_accuracy: 0.7266 - val_loss: 0.1023 - val_clipwise_loss: 0.0681 - val_segmentwise_loss: 0.0683 - val_clipwise_lwlrap: 0.8219 - val_clipwise_accuracy: 0.7336 - val_segmentwise_lwlrap: 0.8251 - val_segmentwise_accuracy: 0.7336\n",
      "Epoch 29/100\n",
      "64/64 - 57s - loss: 0.1145 - clipwise_loss: 0.0739 - segmentwise_loss: 0.0811 - clipwise_lwlrap: 0.5779 - clipwise_accuracy: 0.7720 - segmentwise_lwlrap: 0.5796 - segmentwise_accuracy: 0.7739 - val_loss: 0.0935 - val_clipwise_loss: 0.0618 - val_segmentwise_loss: 0.0635 - val_clipwise_lwlrap: 0.8376 - val_clipwise_accuracy: 0.7582 - val_segmentwise_lwlrap: 0.8432 - val_segmentwise_accuracy: 0.7582\n",
      "Epoch 30/100\n",
      "64/64 - 58s - loss: 0.1187 - clipwise_loss: 0.0762 - segmentwise_loss: 0.0851 - clipwise_lwlrap: 0.5764 - clipwise_accuracy: 0.7588 - segmentwise_lwlrap: 0.5787 - segmentwise_accuracy: 0.7583 - val_loss: 0.0956 - val_clipwise_loss: 0.0630 - val_segmentwise_loss: 0.0651 - val_clipwise_lwlrap: 0.8398 - val_clipwise_accuracy: 0.7500 - val_segmentwise_lwlrap: 0.8442 - val_segmentwise_accuracy: 0.7541\n",
      "Epoch 31/100\n",
      "64/64 - 57s - loss: 0.1122 - clipwise_loss: 0.0724 - segmentwise_loss: 0.0796 - clipwise_lwlrap: 0.5870 - clipwise_accuracy: 0.7671 - segmentwise_lwlrap: 0.5932 - segmentwise_accuracy: 0.7695 - val_loss: 0.0969 - val_clipwise_loss: 0.0631 - val_segmentwise_loss: 0.0676 - val_clipwise_lwlrap: 0.8390 - val_clipwise_accuracy: 0.7541 - val_segmentwise_lwlrap: 0.8382 - val_segmentwise_accuracy: 0.7541\n",
      "Epoch 32/100\n",
      "64/64 - 57s - loss: 0.1083 - clipwise_loss: 0.0701 - segmentwise_loss: 0.0764 - clipwise_lwlrap: 0.5899 - clipwise_accuracy: 0.7842 - segmentwise_lwlrap: 0.5980 - segmentwise_accuracy: 0.7959 - val_loss: 0.0926 - val_clipwise_loss: 0.0611 - val_segmentwise_loss: 0.0630 - val_clipwise_lwlrap: 0.8392 - val_clipwise_accuracy: 0.7582 - val_segmentwise_lwlrap: 0.8406 - val_segmentwise_accuracy: 0.7541\n",
      "Epoch 33/100\n",
      "64/64 - 57s - loss: 0.1092 - clipwise_loss: 0.0701 - segmentwise_loss: 0.0782 - clipwise_lwlrap: 0.5838 - clipwise_accuracy: 0.7964 - segmentwise_lwlrap: 0.5858 - segmentwise_accuracy: 0.7949 - val_loss: 0.0970 - val_clipwise_loss: 0.0640 - val_segmentwise_loss: 0.0660 - val_clipwise_lwlrap: 0.8424 - val_clipwise_accuracy: 0.7664 - val_segmentwise_lwlrap: 0.8448 - val_segmentwise_accuracy: 0.7664\n",
      "Epoch 34/100\n",
      "64/64 - 57s - loss: 0.1003 - clipwise_loss: 0.0647 - segmentwise_loss: 0.0713 - clipwise_lwlrap: 0.5942 - clipwise_accuracy: 0.8115 - segmentwise_lwlrap: 0.5940 - segmentwise_accuracy: 0.8066 - val_loss: 0.0964 - val_clipwise_loss: 0.0625 - val_segmentwise_loss: 0.0679 - val_clipwise_lwlrap: 0.8449 - val_clipwise_accuracy: 0.7623 - val_segmentwise_lwlrap: 0.8547 - val_segmentwise_accuracy: 0.7787\n",
      "Epoch 35/100\n",
      "64/64 - 56s - loss: 0.1009 - clipwise_loss: 0.0653 - segmentwise_loss: 0.0714 - clipwise_lwlrap: 0.6031 - clipwise_accuracy: 0.8179 - segmentwise_lwlrap: 0.6043 - segmentwise_accuracy: 0.8184 - val_loss: 0.0899 - val_clipwise_loss: 0.0595 - val_segmentwise_loss: 0.0608 - val_clipwise_lwlrap: 0.8481 - val_clipwise_accuracy: 0.7746 - val_segmentwise_lwlrap: 0.8494 - val_segmentwise_accuracy: 0.7705\n",
      "Epoch 36/100\n",
      "64/64 - 57s - loss: 0.0992 - clipwise_loss: 0.0638 - segmentwise_loss: 0.0707 - clipwise_lwlrap: 0.5981 - clipwise_accuracy: 0.8198 - segmentwise_lwlrap: 0.6032 - segmentwise_accuracy: 0.8145 - val_loss: 0.0864 - val_clipwise_loss: 0.0578 - val_segmentwise_loss: 0.0573 - val_clipwise_lwlrap: 0.8501 - val_clipwise_accuracy: 0.7746 - val_segmentwise_lwlrap: 0.8567 - val_segmentwise_accuracy: 0.7828\n",
      "Epoch 37/100\n",
      "64/64 - 57s - loss: 0.0983 - clipwise_loss: 0.0631 - segmentwise_loss: 0.0703 - clipwise_lwlrap: 0.5969 - clipwise_accuracy: 0.8184 - segmentwise_lwlrap: 0.6004 - segmentwise_accuracy: 0.8252 - val_loss: 0.0883 - val_clipwise_loss: 0.0579 - val_segmentwise_loss: 0.0607 - val_clipwise_lwlrap: 0.8548 - val_clipwise_accuracy: 0.7787 - val_segmentwise_lwlrap: 0.8612 - val_segmentwise_accuracy: 0.7910\n",
      "Epoch 38/100\n",
      "64/64 - 56s - loss: 0.1000 - clipwise_loss: 0.0646 - segmentwise_loss: 0.0708 - clipwise_lwlrap: 0.5934 - clipwise_accuracy: 0.8140 - segmentwise_lwlrap: 0.5979 - segmentwise_accuracy: 0.8169 - val_loss: 0.0904 - val_clipwise_loss: 0.0605 - val_segmentwise_loss: 0.0600 - val_clipwise_lwlrap: 0.8480 - val_clipwise_accuracy: 0.7705 - val_segmentwise_lwlrap: 0.8567 - val_segmentwise_accuracy: 0.7787\n",
      "Epoch 39/100\n",
      "64/64 - 57s - loss: 0.0938 - clipwise_loss: 0.0603 - segmentwise_loss: 0.0670 - clipwise_lwlrap: 0.6088 - clipwise_accuracy: 0.8296 - segmentwise_lwlrap: 0.6079 - segmentwise_accuracy: 0.8301 - val_loss: 0.0897 - val_clipwise_loss: 0.0600 - val_segmentwise_loss: 0.0595 - val_clipwise_lwlrap: 0.8583 - val_clipwise_accuracy: 0.7828 - val_segmentwise_lwlrap: 0.8710 - val_segmentwise_accuracy: 0.8033\n",
      "Epoch 40/100\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=cfg['training']['n_folds'])\n",
    "random_seeds = [398, 182, 718, 580, 403]\n",
    "val_lwraps = []\n",
    "histories = []\n",
    "print('Configs:', cfg)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(samples, targets)):\n",
    "    K.clear_session()\n",
    "    set_random_seeds(random_seeds[fold])\n",
    "\n",
    "    train_dataset = build_dataset(train_idx, samples, is_train=True)\n",
    "    val_dataset = build_dataset(val_idx, samples, is_train=False)\n",
    "\n",
    "    model = ModelNN()\n",
    "    \n",
    "    print(f'* [{fold}] Train with frozen backbone')\n",
    "    history = model.fit(train_dataset, val_dataset, n_epochs=5, weights_suffix=f'_{fold}_pretrained')\n",
    "    print_val_metrics(history)\n",
    "    \n",
    "    print(f'** [{fold}] Train with full model')\n",
    "    model.load_weights(f'./weights_{fold}_pretrained.h5')\n",
    "    model.unfreeze_backbone()\n",
    "    history = model.fit(train_dataset, val_dataset, n_epochs=100, weights_suffix=f'_{fold}_final')\n",
    "\n",
    "    val_lwrap = max(history.history['val_clipwise_lwlrap'])\n",
    "    val_lwraps.append(val_lwrap)\n",
    "    histories.append(history)\n",
    "    print_val_metrics(history)\n",
    "    \n",
    "print('*** Avg val lwrap:', np.mean(val_lwraps), 'Std:', np.std(val_lwraps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip weights.zip ./weights_*_final.h5\n",
    "!gsutil cp ./weights.zip gs://tn-kaggle-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history, ax=None):\n",
    "    lwlrap = history.history['clipwise_lwlrap']\n",
    "    val_lwlrap = history.history['val_clipwise_lwlrap']\n",
    "    df = pd.DataFrame({'train': lwlrap, 'val': val_lwlrap})\n",
    "    df.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(25, 4))\n",
    "for i, history in enumerate(histories):\n",
    "    ax = plt.subplot(1, len(histories), i + 1)\n",
    "    plot_history(history, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RUN_INFERENCE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_audio(recording_id, train=True):\n",
    "    filepath = os.path.join(TRAIN_INPUT_DIR if train else TEST_INPUT_DIR, recording_id + '.flac')\n",
    "    data, _ = librosa.load(filepath, sr=SR)\n",
    "    return data\n",
    "\n",
    "\n",
    "def cut_audio(audio_data, tmin, tmax, sr=SR, segment_duration=SEGMENT_DURATION):\n",
    "    clip_duration = len(audio_data)/sr\n",
    "    extra_time = max(0, segment_duration - (tmax - tmin)) / 2\n",
    "    tmin = max(0, tmin - extra_time)\n",
    "    tmax = min(clip_duration, tmax + extra_time)\n",
    "\n",
    "    min_sample, max_sample = librosa.time_to_samples([tmin, tmax], sr=sr)\n",
    "    return audio_data[min_sample:(max_sample + 1)] \n",
    "    \n",
    "\n",
    "def get_mel_spec_img(audio_data):\n",
    "    mel_spec = librosa.feature.melspectrogram(audio_data, sr=SR, n_mels=N_MELS)\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "    img = tf.expand_dims(log_mel_spec, -1)\n",
    "    img = tf.image.resize(img, IMG_SIZE[:2])\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    return img, log_mel_spec\n",
    "\n",
    "\n",
    "def cut_and_get_mel_spec_img(audio_data, t_min, t_max):\n",
    "    audio_data = cut_audio(audio_data, t_min, t_max)\n",
    "    return get_mel_spec_img(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(model, recording_id):\n",
    "    audio_data = load_audio(recording_id, train=False)\n",
    "    imgs = []\n",
    "    for t_min in range(0, 55, cfg['inference']['segment_stride']):\n",
    "        t_max = t_min + SEGMENT_DURATION\n",
    "        img, _ = cut_and_get_mel_spec_img(audio_data, t_min, t_max)\n",
    "        img = preprocess_img(img, False)\n",
    "        imgs.append(img)\n",
    "    preds = model.predict(tf.stack(imgs))\n",
    "    return tf.reduce_max(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if RUN_INFERENCE:\n",
    "    model = ModelNN()\n",
    "    model.load_weights('./weights.h5')\n",
    "    preds = predict(model, submission['recording_id'][0])\n",
    "    submission.iloc[0, 1:] = preds.numpy()\n",
    "    submission.to_csv('./submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
